{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOc+nvxXYBREk7S39fbOTWH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QwertyJacob/colab_handouts_PSI/blob/main/random_vars1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variabili Aleatorie e Valori Attesi\n",
        "\n",
        "Abbiamo gli strumenti per descrivere esperimenti con risultati casuali, ma siamo principalmente interessati ai numeri che sono casuali. È semplice collegare un numero al risultato di un esperimento. Il risultato è una variabile casuale, una nuova idea utile.\n",
        "\n",
        "Le variabili casuali si presentano in ogni tipo di situazione. Per esempio, la quantità di denaro che vinci o perdi in una scommessa è una variabile casuale. Se fai la stessa scommessa ripetutamente, potresti chiederti quanto denaro cambierà di mano in totale, per scommessa. Questo porta a una nuova e utile idea: il valore atteso di una variabile casuale.\n",
        "\n",
        "##  Variabili Aleatorie\n",
        "\n",
        "Molto comunemente, vorremmo trattare con numeri che sono casuali. Possiamo farlo collegando numeri al risultato di un esperimento. Definiamo una variabile casuale:\n",
        "\n",
        "**Definizione** (Variabile Casuale Discreta) Dato uno spazio campionario $\\Omega$, e un insieme numerabile di numeri reali $D$, una variabile aleatoria è una funzione con dominio $\\Omega$ e codominio $D$. (Essa associa ad ogni elemento di $\\Omega$ un numero in $D$.\n",
        "\n",
        "Questo significa che per ogni risultato $\\omega$ c'è un numero associato ad esso tramite la variabile aleatoria $X$, che potremmo scrivere come $X(\\omega)$. Nota: non stiamo parlando della probabilità dell'esito, ma di un'altro numero, qualsiasi, che viene associato all'occorrenza dell'esito in questione.\n",
        "\n",
        "**Esempio** (Numeri da Monete) Lanciamo una moneta. Ogni volta che la moneta esce testa, registriamo 1; quando esce croce, registriamo 0. Abbiamo creato una variabile aleatoria! Cioè abbiamo creato una funzione che associa un numero ad ogni esito in $\\Omega$. Così semplice ;)\n",
        "\n",
        "**Esempio** (Numeri da Monete II) Lanciamo una moneta 32 volte. Registriamo 1 quando esce testa, e quando esce croce, registriamo 0. Questo produce una sequenza di 32 bit (0 oppure 1). Se poi trasformiamo questo numero a base decimale positivo otteniamo un numero tra 0 e $2^{31-1}$, giusto?. Quindi queste sequenze possono essere viste come numeri, quindi abbiamo creato una variabile aleatoria associata ad un certo esperimento aleatorio (diverso da quello precedente).\n",
        "\n",
        "**Esempio** (Il Numero di Coppie in una Mano di Poker) Peschiamo una mano di cinque carte. Modelliamo il numero di coppie in questa mano è una variabile aleatoria, che assume i valori 0, 1, 2 (a seconda della mano che peschiamo).\n",
        "\n",
        "Nota: una funzione che prende una variabile aleatoria discreta e restituisce un insieme di numeri è anche una variabile casuale discreta. **Esempio** (Parità dei Lanci di Moneta) Lanciamo una moneta 32 volte. Registriamo 1 quando esce testa, e quando esce croce, registriamo 0. Abbiamo visto che questa sequenza di bit può essere mappata ad un numero casuale tra 0 e $2^{32-1}$, e che quindi è una variabile aleatoria. Poi possiamo creare un'altra V.A. associata al fatto che quel numero sia pari o meno...\n",
        "\n",
        "> In teoria, avendo dato la defizione di V.A di sopra, possiamo trovare associati a ogni numero $x$ più di un esito! Quindi, per correttezza focalizziamoci sull'evento (cioè un potenziale insieme di esiti $\\omega_i$ tali che $X(\\omega_i) = x$. Questo evento si può denotare con $\\{\\omega : X(\\omega) = x\\}$; oppure in $\\{X = x\\}$.\n",
        "\n",
        "Ora parliamo di **probabilità**: La probabilità che una variabile casuale $X$ assuma il valore $x$ è data dalla probabilità che si verifichi l'evento $\\{\\omega : X(\\omega) = x\\}$, Cioè $P(\\{\\omega : X(\\omega) = x\\})$. Questa probabilità è più comunemente scritta $P(\\{X = x\\})$, ma ancor più comunemente scritta così:  $P(X = x)$, e, moltissime volte, per brevità, anche così: $P(x)$.\n",
        "\n",
        "> Quindi non farti fregare! il fatto che una v.a. assuma un certo valore è un evento, oppure rappresenta un evento, che ha una certa probabilità. Se poi questo evento fa riferimento ad un singolo esito oppure ad un insieme di molti esiti, dipenderà di come abbiamo modellato la v.a. in questione.\n",
        "\n",
        "**Definizione** (Distribuzione di Probabilità di una Variabile Aleatoria Discreta) La distribuzione di probabilità di una variabile casuale discreta è l'insieme dei numeri $P(\\{X = x\\})$ per ogni valore $x$ che $X$ può assumere. La distribuzione assume il valore 0 per tutti gli altri numeri. Si noti che la distribuzione è non negativa. La distribuzione di probabilità è talvolta nota come funzione di massa di probabilità. Anche abbreviata con la sigla PMF (dall'inglese _probability mass function_).\n",
        "\n",
        "**Esempio Svolto** (Numeri da Monete III) Lanciamo una moneta truccata 2 volte. I lanci sono indipendenti. La moneta ha $P(T) = p$, $P(C) = 1-p$. Registriamo 1 quando esce testa, e quando esce croce, registriamo 0. Questo produce un numero casuale a 2 bit, che, convertita a base decimale, è una variabile casuale che assume i valori 0, 1, 2, 3. Qual è la distribuzione di probabilità di questa variabile casuale?\n",
        "\n",
        "*Soluzione* Distribuzione di probabilità:\n",
        "- $P(0) = (1-p)^2$\n",
        "- $P(1) = (1-p)p$\n",
        "- $P(2) = p(1-p)$\n",
        "- $P(3) = p^2$\n",
        "\n",
        "\n",
        "\n",
        "**Esempio Svolto** (Scommettere sulle Monete) Un modo per ottenere una variabile casuale è pensare alla ricompensa di una scommessa. Concordiamo di giocare il seguente gioco. Lancio una moneta. La moneta ha $P(T) = p$, $P(C) = 1-p$. Se la moneta esce testa, mi paghi $q$; se la moneta esce croce, ti pago $r$. Il numero di dollari che cambiano di mano è una variabile casuale. Qual è la sua distribuzione di probabilità?\n",
        "\n",
        "*Soluzione* Vediamo questo problema dalla mia prospettiva. Se la moneta esce testa, ottengo $q$; se esce croce, ottengo $-r$. Quindi abbiamo $P(X = q) = p$ e $P(X = -r) = (1-p)$, e tutte le altre probabilità sono zero.\n"
      ],
      "metadata": {
        "id": "z7EzKGq40d-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Il Valore Atteso e le Sue Proprietà Fondamentali\n",
        "\n",
        "## Definizione del Valore Atteso per Variabili Aleatorie Discrete\n",
        "\n",
        "Data una variabile aleatoria discreta $X$ che assume valori in un insieme numerabile $D$ con funzione di massa di probabilità $P(X = x)$, il valore atteso è definito come:\n",
        "\n",
        "$$\\mathbb{E}[X] = \\sum_{x \\in D} x P(X = x)$$\n",
        "\n",
        "dove la somma è estesa a tutti i possibili valori che la variabile aleatoria può assumere.\n",
        "\n",
        "### Osservazioni Importanti:\n",
        "1. Il valore atteso potrebbe non esistere se la serie non converge assolutamente\n",
        "2. Il valore atteso può non appartenere all'insieme dei valori possibili della variabile aleatoria\n",
        "3. $\\mathbb{E}[X]$ rappresenta il \"centro di massa\" della distribuzione di probabilità\n",
        "\n",
        "## Proprietà di Linearità\n",
        "\n",
        "Il valore atteso gode di importanti proprietà lineari:\n",
        "\n",
        "### 1. Linearità rispetto alla costante\n",
        "Per qualsiasi costante $a \\in \\mathbb{R}$:\n",
        "\n",
        "$$\\mathbb{E}[aX] = a\\mathbb{E}[X]$$\n",
        "\n",
        "### 2. Linearità rispetto alla somma\n",
        "Per qualsiasi costante $b \\in \\mathbb{R}$:\n",
        "\n",
        "$$\\mathbb{E}[X + b] = \\mathbb{E}[X] + b$$\n",
        "\n",
        "### 3. Linearità generale\n",
        "Per qualsiasi costante $a,b \\in \\mathbb{R}$:\n",
        "\n",
        "$$\\mathbb{E}[aX + b] = a\\mathbb{E}[X] + b$$\n",
        "\n",
        "## Valore Atteso della Somma di Variabili Aleatorie\n",
        "\n",
        "Per due variabili aleatorie $X$ e $Y$, vale la proprietà:\n",
        "\n",
        "$$\\mathbb{E}[X + Y] = \\mathbb{E}[X] + \\mathbb{E}[Y]$$\n",
        "\n",
        "Questa proprietà è vera indipendentemente dalla dipendenza o indipendenza tra $X$ e $Y$.\n",
        "\n",
        "### Caso Generale\n",
        "Per una somma finita di variabili aleatorie $X_1, X_2, \\ldots, X_n$:\n",
        "\n",
        "$$\\mathbb{E}\\left[\\sum_{i=1}^n X_i\\right] = \\sum_{i=1}^n \\mathbb{E}[X_i]$$\n",
        "\n",
        "## Legge dello Statistico Inconscio (Law of the Unconscious Statistician - LOTUS)\n",
        "\n",
        "La legge dello statistico inconscio, o LOTUS, ci permette di calcolare il valore atteso di una funzione di una variabile aleatoria senza dover prima trovare la distribuzione della funzione stessa.\n",
        "\n",
        "Per una funzione $g$ e una variabile aleatoria discreta $X$:\n",
        "\n",
        "$$\\mathbb{E}[g(X)] = \\sum_{x \\in D} g(x)P(X = x)$$\n",
        "\n",
        "### Importanza della LOTUS\n",
        "Questa legge è particolarmente utile quando:\n",
        "1. Vogliamo calcolare il valore atteso di una trasformazione di $X$\n",
        "2. Non è pratico o possibile trovare la distribuzione di $g(X)$\n",
        "3. Abbiamo già la distribuzione di $X$\n",
        "\n",
        "### Esempio di Applicazione\n",
        "Se volessimo calcolare $\\mathbb{E}[X^2]$ per una variabile aleatoria discreta $X$:\n",
        "\n",
        "$$\\mathbb{E}[X^2] = \\sum_{x \\in D} x^2 P(X = x)$$\n",
        "\n",
        "anziché dover prima trovare la distribuzione di $X^2$.\n",
        "\n",
        "## Note Finali\n",
        "\n",
        "1. Tutte queste proprietà sono fondamentali per il calcolo pratico dei valori attesi\n",
        "2. Le proprietà di linearità rendono il valore atteso uno strumento molto utile nell'analisi probabilistica\n",
        "3. La LOTUS semplifica notevolmente molti calcoli che altrimenti sarebbero più complessi\n",
        "4. Queste proprietà si estendono anche al caso di variabili aleatorie continue, con le somme sostituite da integrali"
      ],
      "metadata": {
        "id": "4BX-x6yqAQVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Esempi sul Valore Atteso\n",
        "\n",
        "Parliamo ancora dell'esempio precedente. La moneta ha $P(T) = p$, $P(C) = 1-p$. Se la moneta esce testa, mi paghi $q$; se la moneta esce croce, ti pago $r$. Ora immagina che giochiamo questo gioco molte volte. La nostra definizione frequentista della probabilità implica che in $N$ giochi, ci aspettiamo di vedere circa $pN$ teste e $(1-p)N$ croci. A sua volta, questo significa che il mio reddito totale da questi $N$ giochi dovrebbe essere circa $(pN)q - ((1-p)N)r$. La $N$ in questa espressione è scomoda; invece, potremmo dire che per ogni singolo gioco, il mio reddito atteso è:\n",
        "\n",
        "$pq - (1-p)r$\n",
        "\n",
        "Questo non è il reddito effettivo da un singolo gioco (che sarebbe o $q$ o $r$, a seconda di cosa ha fatto la moneta). È invece una stima di quello che accadrebbe su un gran numero di giochi, su base _per-gioco_. Questo è un esempio di un valore atteso.\n",
        "\n",
        "> Si noti che il valore atteso potrebbe assumere un valore che la variabile casuale non assume. **Esempio** (Scommettere sulle Monete) Concordiamo di giocare il seguente gioco. Lancio una moneta equa (cioè $P(T) = P(C) = 1/2$). Se la moneta esce testa, mi paghi 1; se la moneta esce croce, ti pago 1. Il valore atteso del mio guadagno per gioco è 0, anche se la variabile casuale non assume mai quel valore!\n",
        "\n",
        "**Esempio Svolto** (Scommettere sulle Monete, Ancora) Concordiamo di giocare il seguente gioco. Lancio una moneta equa (cioè $P(T) = P(C) = 1/2$). Se la moneta esce testa, mi paghi 2; se la moneta esce croce, ti pago 1. Qual è il valore atteso di questo gioco?\n",
        "\n",
        "*Soluzione* Il valore atteso del mio guadagno è:\n",
        "\n",
        "$(1/2) \\cdot 2 - (1/2) \\cdot 1 = 1/2$\n",
        "\n",
        "Si noti che questo non è nemmeno un numero intero, e non c'è modo che una singola istanza del gioco possa dare un guadagno di $1/2$. Ma questo è quello che otterrei, per gioco, se giocassi molte volte.\n",
        "\n",
        "La tua intuizione probabilmente ti dirà che il gioco dell'esempio appena fatto è buono per me e cattivo per te. Questa intuizione è corretta. Si scopre che è possibile fare un'affermazione ancora più forte: giocare questo gioco ripetutamente è praticamente garantito essere eccellente per me e disastroso per te. Vedremo dopo come esattamente si dimostra questo."
      ],
      "metadata": {
        "id": "CEzfTqch8NUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# La Varianza di una Variabile Aleatoria\n",
        "\n",
        "## Definizione\n",
        "\n",
        "La varianza di una variabile aleatoria $X$ è una misura della sua dispersione attorno al valore atteso $\\mathbb{E}[X]$. È definita come:\n",
        "\n",
        "$$\\text{Var}[X] = \\mathbb{E}[(X - \\mathbb{E}[X])^2]$$\n",
        "\n",
        "Intuitivamente, misura la media dei quadrati delle deviazioni dal valore atteso.\n",
        "\n",
        "## Proprietà Fondamentali\n",
        "\n",
        "1. **Non-negatività**:\n",
        "   $$\\text{Var}[X] \\geq 0$$\n",
        "\n",
        "2. **Varianza di una costante**:\n",
        "   $$\\text{Var}[c] = 0$$\n",
        "   dove $c$ è una costante\n",
        "\n",
        "3. **Effetto di una costante moltiplicativa**:\n",
        "   $$\\text{Var}[aX] = a^2\\text{Var}[X]$$\n",
        "   dove $a$ è una costante\n",
        "\n",
        "4. **Effetto di una traslazione**:\n",
        "   $$\\text{Var}[X + b] = \\text{Var}[X]$$\n",
        "   dove $b$ è una costante\n",
        "\n",
        "5. **Varianza della somma di variabili aleatorie indipendenti**:\n",
        "   $$\\text{Var}[X + Y] = \\text{Var}[X] + \\text{Var}[Y]$$\n",
        "   (questa proprietà vale solo se $X$ e $Y$ sono indipendenti)\n",
        "\n",
        "## Dimostrazione della Formula Alternativa\n",
        "\n",
        "Dimostriamo ora che $\\text{Var}[X] = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2$\n",
        "\n",
        "* Passo 1: Partiamo dalla definizione\n",
        "$$\\text{Var}[X] = \\mathbb{E}[(X - \\mathbb{E}[X])^2]$$\n",
        "\n",
        "* Passo 2: Espandiamo il quadrato\n",
        "$$(X - \\mathbb{E}[X])^2 = X^2 - 2X\\mathbb{E}[X] + \\mathbb{E}[X]^2$$\n",
        "\n",
        "* Passo 3: Applichiamo il valore atteso\n",
        "$$\\text{Var}[X] = \\mathbb{E}[X^2 - 2X\\mathbb{E}[X] + \\mathbb{E}[X]^2]$$\n",
        "\n",
        "* Passo 4: Usiamo la linearità del valore atteso\n",
        "$$\\text{Var}[X] = \\mathbb{E}[X^2] - 2\\mathbb{E}[X]\\mathbb{E}[X] + \\mathbb{E}[X]^2$$\n",
        "\n",
        "* Passo 5: Semplifichiamo\n",
        "$$\\text{Var}[X] = \\mathbb{E}[X^2] - 2(\\mathbb{E}[X])^2 + (\\mathbb{E}[X])^2 = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$$\n",
        "\n",
        "## Esempi di Calcolo della Varianza\n",
        "\n",
        "### Esempio 1: Lancio di un dado equo\n",
        "Consideriamo una variabile aleatoria $X$ che rappresenta il risultato del lancio di un dado equo a 6 facce.\n",
        "\n",
        "1. Calcoliamo $\\mathbb{E}[X]$:\n",
        "   $$\\mathbb{E}[X] = \\sum_{i=1}^6 i \\cdot \\frac{1}{6} = \\frac{21}{6} = 3.5$$\n",
        "\n",
        "2. Calcoliamo $\\mathbb{E}[X^2]$:\n",
        "   $$\\mathbb{E}[X^2] = \\sum_{i=1}^6 i^2 \\cdot \\frac{1}{6} = \\frac{91}{6} \\approx 15.167$$\n",
        "\n",
        "3. Quindi la varianza è:\n",
        "   $$\\text{Var}[X] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = \\frac{91}{6} - (\\frac{21}{6})^2 = \\frac{35}{12} \\approx 2.917$$\n",
        "\n",
        "### Esempio 2: Variabile aleatoria di Bernoulli\n",
        "Sia $X$ una variabile aleatoria di Bernoulli con parametro $p$ (successo con probabilità $p$, insuccesso con probabilità $1-p$).\n",
        "\n",
        "1. Sappiamo che:\n",
        "   $$\\mathbb{E}[X] = p$$\n",
        "\n",
        "2. Poiché $X$ può assumere solo i valori 0 e 1:\n",
        "   $$\\mathbb{E}[X^2] = 0^2 \\cdot (1-p) + 1^2 \\cdot p = p$$\n",
        "\n",
        "3. Quindi:\n",
        "   $$\\text{Var}[X] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = p - p^2 = p(1-p)$$\n",
        "\n",
        "## Note Importanti\n",
        "\n",
        "1. La varianza ha unità di misura quadratiche rispetto alla variabile originale\n",
        "2. Per questo motivo, spesso si usa la deviazione standard $ \\text{Std}[X] = \\sqrt{\\text{Var}[X]}$\n",
        "3. La formula $\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$ è spesso più pratica per i calcoli\n",
        "4. La varianza è particolarmente utile nello studio di somme di variabili aleatorie indipendenti"
      ],
      "metadata": {
        "id": "H4j7_n1BB1b_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# La Distribuzione Binomiale\n",
        "\n",
        "Immaginiamo di voler lanciare una moneta 3 volte e di voler contare il numero di teste.\n",
        "\n",
        "* Comprendiamo il Processo\n",
        "  - Ogni lancio è indipendente\n",
        "  - La probabilità di testa è $p$ (successo)\n",
        "  - La probabilità di croce è $1-p$ (insuccesso)\n",
        "  - Parliamo dell'evento \"ottenere $k$ successi in $n$ lanci\"\n",
        "\n",
        "* Iniziamo con un Caso Concreto: Supponiamo di volere esattamente 2 teste in 3 lanci.Come può succedere?\n",
        "    1. Testa, Testa, Croce (TTC)\n",
        "    2. Testa, Croce, Testa (TCT)\n",
        "    3. Croce, Testa, Testa (CTT)\n",
        "\n",
        "Notiamo che:\n",
        "    - Ogni sequenza ha la stessa probabilità\n",
        "    - Stiamo parlando di esiti diversi.\n",
        "    - Tutte queste sequenze fanno riferimento all'evento \"otteniamo 2 teste in 3 lanci\".\n",
        "\n",
        "Calcoliamo la Probabilità di una Sequenza Specifica\n",
        "Prendiamo TTC come esempio:\n",
        "  - Primo lancio (T): probabilità $p$\n",
        "  - Secondo lancio (T): probabilità $p$\n",
        "  - Terzo lancio (C): probabilità $(1-p)$\n",
        "\n",
        "Probabilità totale della sequenza = $p \\cdot p \\cdot (1-p) = p^2(1-p)$. Non è difficile rendersi conto che questa probabilità vale per tutte e tre le sequenze di sopra, quindi la probabilità dell'evento \"otteniamo 2 teste in 3 lanci\" è: $3 \\cdot p^2(1-p)$\n",
        "\n",
        "> Riconosciamo il Pattern Il numero 3 non è casuale. È il numero di modi di scegliere 2 posizioni da 3 possibili! Oppure, può anche essere inteso come il numero di disposizioni di 3 elementi dove ci sono due classi di elementi indistinguibili (T e C) e dove ci sono 2 elementi di tipo T e 1 di tipo C. Vi ricordate il caso degli anagrammi, sì?! Insomma, il numero che cerchiamo lo abbiamo usando:\n",
        "$$\\frac{3!}{2!1!}=\\binom{3}{2} = 3$$\n",
        "\n",
        "* Generalizziamo. Per $k$ successi in $n$ prove:\n",
        "  1. Ogni sequenza specifica ha probabilità $p^k(1-p)^{n-k}$\n",
        "  2. Il numero di possibili sequenze è $\\binom{n}{k}$\n",
        "\n",
        "Quindi la probabilità totale è:\n",
        "\n",
        "$$P(X = k) = \\binom{n}{k}p^k(1-p)^{n-k}$$\n",
        "\n",
        "* Per chiudere il cerchio, calcoliamo la probabilità di ottenere esattamente 2 teste in 3 lanci con una moneta equa ($p = 1/2$):\n",
        "\n",
        "1. Calcoliamo $\\binom{3}{2} = 3$\n",
        "2. $p^2 = (1/2)^2 = 1/4$\n",
        "3. $(1-p)^{3-2} = (1/2)^1 = 1/2$\n",
        "\n",
        "Quindi:\n",
        "$$P(X = 2) = 3 \\cdot \\frac{1}{4} \\cdot \\frac{1}{2} = \\frac{3}{8}$$\n",
        "\n",
        "Si noti che, data l'assunzione di equiprobabilità dei risultati, il risultato coincide con il calcolo del semplice rapporto tra casi favorevoli e casi totali!\n",
        "\n",
        "## Proprietà della distribuzione Binomiale:\n",
        "1. **Media**: $\\mathbb{E}[X] = np$\n",
        "2. **Varianza**: $\\text{Var}[X] = np(1-p)$\n",
        "\n",
        "Note:\n",
        "1. La somma delle probabilità per tutti i possibili valori di $k$ è 1:\n",
        "   $$\\sum_{k=0}^n \\binom{n}{k}p^k(1-p)^{n-k} = 1$$\n",
        "   (Questo è il teorema binomiale!)\n",
        "\n",
        "2. I casi estremi hanno senso:\n",
        "   - $P(X = 0) = (1-p)^n$ (tutti insuccessi)\n",
        "   - $P(X = n) = p^n$ (tutti successi)\n",
        "\n",
        "La distribuzione binomiale è un modello fondamentale perché emerge ogni volta che contiamo successi in prove indipendenti."
      ],
      "metadata": {
        "id": "mb8uPl5PDE0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## La Variabile Aleatoria di Bernoulli\n",
        "\n",
        "La variabile aleatoria di Bernoulli è il caso più semplice di variabile aleatoria discreta e descrive un esperimento con due soli possibili esiti: successo (1) o insuccesso (0).\n",
        "\n",
        "### Definizione Formale\n",
        "Una variabile aleatoria $X$ si dice di Bernoulli se:\n",
        "- $X$ può assumere solo i valori 0 e 1\n",
        "- $P(X = 1) = p$, dove $p \\in [0,1]$ è il parametro della distribuzione\n",
        "- $P(X = 0) = 1 - p$\n",
        "\n",
        "La funzione di probabilità può essere scritta in forma compatta come:\n",
        "$$P(X = k) = p^k(1-p)^{1-k}, \\quad k \\in \\{0,1\\}$$\n",
        "\n",
        "### Caratteristiche\n",
        "- **Valore Atteso**: $\\mathbb{E}[X] = p$\n",
        "- **Varianza**: $\\text{Var}(X) = p(1-p)$\n",
        "\n",
        "## Generalizzazione alla Variabile Aleatoria Binomiale\n",
        "\n",
        "La variabile aleatoria binomiale può essere vista come una naturale estensione della variabile di Bernoulli: rappresenta la somma di $n$ prove di Bernoulli indipendenti e identicamente distribuite (i.i.d.).\n",
        "\n",
        "### Il Collegamento tra Bernoulli e Binomiale\n",
        "\n",
        "Se consideriamo una sequenza di $n$ esperimenti di Bernoulli indipendenti, ciascuno con probabilità di successo $p$, e definiamo:\n",
        "- $X_1, X_2, ..., X_n$ come le singole prove di Bernoulli\n",
        "- $Y = \\sum_{i=1}^n X_i$ come il numero totale di successi\n",
        "\n",
        "Allora $Y$ segue una distribuzione binomiale con parametri $n$ e $p$, che scriviamo come $Y \\sim B(n,p)$.\n",
        "\n",
        "La funzione di probabilità della binomiale è:\n",
        "$$P(Y = k) = \\binom{n}{k}p^k(1-p)^{n-k}, \\quad k \\in \\{0,1,...,n\\}$$\n",
        "\n",
        "### Dimostrazione della Generalizzazione\n",
        "\n",
        "1. Ogni singola prova $X_i$ è una Bernoulli$(p)$\n",
        "2. La somma $Y$ rappresenta il conteggio dei successi in $n$ prove\n",
        "3. Per le proprietà di indipendenza:\n",
        "   - $\\mathbb{E}[Y] = \\mathbb{E}[\\sum_{i=1}^n X_i] = \\sum_{i=1}^n \\mathbb{E}[X_i]$\n",
        "   - $\\text{Var}(Y) = \\text{Var}(\\sum_{i=1}^n X_i) = \\sum_{i=1}^n \\text{Var}(X_i)$\n",
        "\n",
        "Questo ci porta alle formule per la binomiale:\n",
        "- $\\mathbb{E}[Y] = np$\n",
        "- $\\text{Var}(Y) = np(1-p)$\n",
        "\n",
        "che sono chiaramente una generalizzazione delle formule della Bernoulli per $n=1$.\n",
        "\n",
        "### Osservazioni Importanti\n",
        "\n",
        "1. La v.a. di Bernoulli è un caso particolare della binomiale quando $n=1$\n",
        "2. La binomiale eredita e amplifica le proprietà statistiche della Bernoulli\n",
        "3. Le proprietà di indipendenza sono fondamentali:\n",
        "   $$P(X_i = 1 | X_j = k) = P(X_i = 1) = p, \\quad \\forall i \\neq j$$\n",
        "\n",
        "Questa generalizzazione è fondamentale per:\n",
        "- L'analisi di sequenze di prove ripetute\n",
        "- Lo studio di fenomeni discreti in vari ambiti applicativi\n",
        "- La modellazione di processi con esiti binari multipli"
      ],
      "metadata": {
        "id": "tNCI9qeZHbZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Materiale consigliato:**\n",
        "_____________\n",
        "Prova a dare un occhiata alle [dispense](https://colab.research.google.com/github/ccaudek/ds4psy_2023/blob/main/205_discr_rv_distr.ipynb) di \"psicometria\" del prof. Corrado Caudek."
      ],
      "metadata": {
        "id": "Cgls_cfQFtWG"
      }
    }
  ]
}
