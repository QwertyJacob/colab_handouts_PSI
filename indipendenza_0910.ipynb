{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP16dDikyFIu6TrbA26wOGo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QwertyJacob/colab_handouts_PSI/blob/main/indipendenza_0910.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# La Probabilità Condizionata\n"
      ],
      "metadata": {
        "id": "lMQQnaeMxxhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "La probabilità condizionata è un concetto fondamentale che ci permette di rispondere a domande del tipo: \"Qual è la probabilità di un evento, dato che sappiamo che un altro evento è già accaduto?\"\n",
        "\n",
        "## Definizione Formale\n",
        "\n",
        "Dati due eventi $\\mathcal{A}$ e $\\mathcal{B}$, la probabilità condizionata di $\\mathcal{A}$ dato $\\mathcal{B}$ si denota come $P(\\mathcal{A}|\\mathcal{B})$ e si calcola come:\n",
        "\n",
        "$$P(\\mathcal{A}|\\mathcal{B}) = \\frac{P(\\mathcal{A} \\cap \\mathcal{B})}{P(\\mathcal{B})}$$\n",
        "\n",
        "dove $P(\\mathcal{B}) > 0$\n",
        "\n",
        "## Interpretazione Intuitiva\n",
        "\n",
        "### Diagrammi di Venn\n",
        "\n",
        "I diagrammi di Venn forniscono un'interpretazione visiva molto intuitiva della probabilità condizionata:\n",
        "\n",
        "1. L'intero rettangolo rappresenta lo spazio campionario $\\Omega$\n",
        "2. Quando condizioniamo su $\\mathcal{B}$, è come se \"zoomassimo\" sull'area di $\\mathcal{B}$\n",
        "3. $P(\\mathcal{A}|\\mathcal{B})$ è la proporzione dell'area di $\\mathcal{B}$ che è anche in $\\mathcal{A}$\n",
        "\n",
        "### Interpretazione Frequentista\n",
        "\n",
        "Pensando alla definizione frequentista di probabilità:\n",
        "- Se in 100 lanci di due dadi, otteniamo 36 volte un totale pari ($\\mathcal{B}$)\n",
        "- E di queste 36 volte, 18 volte il primo dado era 6 ($\\mathcal{A}$)\n",
        "- Allora $P(\\mathcal{A}|\\mathcal{B}) = \\frac{18}{36} = 0.5$\n",
        "\n",
        "## Proprietà Fondamentali\n",
        "\n",
        "### Assiomi della Probabilità nel Nuovo Spazio\n",
        "\n",
        "Quando condizioniamo su $\\mathcal{B}$, creiamo un nuovo spazio di probabilità dove:\n",
        "\n",
        "1. $P(\\mathcal{B}|\\mathcal{B}) = 1$ (certezza)\n",
        "2. Per eventi disgiunti $\\mathcal{C}$ e $\\mathcal{D}$:\n",
        "   $$P(\\mathcal{C} \\cup \\mathcal{D}|\\mathcal{B}) = P(\\mathcal{C}|\\mathcal{B}) + P(\\mathcal{D}|\\mathcal{B})$$\n",
        "\n",
        "### Conservazione delle Proporzioni\n",
        "\n",
        "Un aspetto fondamentale è che le proporzioni relative tra gli eventi si mantengono nel nuovo spazio condizionato:\n",
        "\n",
        "Per qualsiasi evento $\\mathcal{E}$:\n",
        "$$\\frac{P(\\mathcal{E} \\cap \\mathcal{B})}{P(\\mathcal{B})} = P(\\mathcal{E}|\\mathcal{B})$$\n",
        "\n",
        "Questo significa che le \"proporzioni\" degli eventi all'interno di $\\mathcal{B}$ rimangono le stesse, solo _riscalate_ per sommare a 1.\n",
        "\n",
        "## Esempio Pratico\n",
        "\n",
        "Consideriamo un mazzo di 52 carte:\n",
        "- Sia $\\mathcal{A}$ l'evento \"estrarre un asso\"\n",
        "- Sia $\\mathcal{B}$ l'evento \"estrarre una carta di cuori\"\n",
        "\n",
        "Allora:\n",
        "- $P(\\mathcal{A}) = \\frac{4}{52} = \\frac{1}{13}$\n",
        "- $P(\\mathcal{B}) = \\frac{13}{52} = \\frac{1}{4}$\n",
        "- $P(\\mathcal{A} \\cap \\mathcal{B}) = \\frac{1}{52}$ (l'asso di cuori)\n",
        "\n",
        "La probabilità di estrarre un asso, dato che sappiamo che la carta è di cuori:\n",
        "\n",
        "$$P(\\mathcal{A}|\\mathcal{B}) = \\frac{P(\\mathcal{A} \\cap \\mathcal{B})}{P(\\mathcal{B})} = \\frac{\\frac{1}{52}}{\\frac{13}{52}} = \\frac{1}{13}$$\n",
        "\n",
        "Questo risultato è intuitivo: tra le 13 carte di cuori, solo una è un asso."
      ],
      "metadata": {
        "id": "jO94jaBlqjeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eventi Dipendenti e Indipendenti\n"
      ],
      "metadata": {
        "id": "aAOJX5Xvxs_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Due eventi $A$ e $B$ in uno spazio campionario $\\Omega$ si dicono **indipendenti** se il verificarsi di uno non influenza la probabilità del verificarsi dell'altro.\n",
        "\n",
        "$\\mathcal{A}$ e $\\mathcal{B}$ sono indipendenti se e solo se:\n",
        "\n",
        "$$P(\\mathcal{A}|\\mathcal{B}) = P(\\mathcal{A})$$\n",
        "\n",
        "o equivalentemente:\n",
        "\n",
        "$$P(\\mathcal{A} \\cap \\mathcal{B}) = P(\\mathcal{A}) \\cdot P(\\mathcal{B})$$\n",
        "\n",
        "### Ragionamento Intuitivo\n",
        "\n",
        "Per capire se due eventi sono indipendenti, chiediamoci:\n",
        "1. Il verificarsi di un evento cambia la probabilità dell'altro?\n",
        "2. Le informazioni su un evento ci dicono qualcosa sull'altro?\n",
        "\n",
        "## Esempi di Eventi Indipendenti\n",
        "\n",
        "1. **Lancio di due dadi**\n",
        "   - Evento $\\mathcal{A}$: ottenere 6 sul primo dado\n",
        "   - Evento $\\mathcal{B}$: ottenere un numero pari sul secondo dado\n",
        "   - Sono indipendenti perché il risultato di un dado non influenza l'altro\n",
        "\n",
        "2. **Estrazioni con reimmissione**\n",
        "   - Evento $\\mathcal{A}$: estrarre una pallina rossa\n",
        "   - Rimettere la pallina nell'urna\n",
        "   - Evento $\\mathcal{B}$: estrarre una pallina blu\n",
        "   - Sono indipendenti perché la prima estrazione non cambia la composizione dell'urna\n",
        "\n",
        "3. **Meteorologia e Matematica**\n",
        "   - Evento $\\mathcal{A}$: piove domani\n",
        "   - Evento $\\mathcal{B}$: uno studente supera un esame di matematica\n",
        "   - Sono indipendenti perché non c'è alcuna relazione causale tra i due eventi\n",
        "\n",
        "## Esempi di Eventi Dipendenti\n",
        "\n",
        "1. **Estrazioni senza reimmissione**\n",
        "   - Evento $\\mathcal{A}$: estrarre un asso da un mazzo di carte\n",
        "   - Evento $\\mathcal{B}$: estrarre un altro asso dallo stesso mazzo\n",
        "   - Sono dipendenti perché la prima estrazione cambia la probabilità della seconda\n",
        "\n",
        "2. **Test medici**\n",
        "   - Evento $\\mathcal{A}$: avere una certa malattia\n",
        "   - Evento $\\mathcal{B}$: risultato positivo al test per quella malattia\n",
        "   - Sono dipendenti perché avere la malattia influenza fortemente la probabilità di un test positivo\n",
        "\n",
        "3. **Rendimento scolastico**\n",
        "   - Evento $\\mathcal{A}$: studiare per un esame\n",
        "   - Evento $\\mathcal{B}$: superare quell'esame\n",
        "   - Sono dipendenti perché studiare aumenta la probabilità di superare l'esame\n",
        "\n",
        "## Eventi Disgiunti (Mutuamente Esclusivi) e dipendenza\n",
        "\n",
        "Sappiamo che due eventi sono disgiunti se non possono verificarsi contemporaneamente. Si ha che eventi disgiunti sono sempre dipendenti. Perché?\n",
        "\n",
        "1. **Guardiamo i numeri**\n",
        "   - Eventi disgiunti: $P(\\mathcal{A} \\cap \\mathcal{B}) = 0$\n",
        "   - Eventi indipendenti: $P(\\mathcal{A} \\cap \\mathcal{B}) = P(\\mathcal{A}) \\cdot P(\\mathcal{B})$\n",
        "   - Se entrambe fossero vere, dovremmo avere $P(\\mathcal{A}) = 0$ o $P(\\mathcal{B}) = 0$\n",
        "\n",
        "2. **Ragionamento intuitivo**\n",
        "   - Per come sono definiti gli eventi disgiunti, se so che si è verificato $\\mathcal{A}$, so con certezza che $\\mathcal{B}$ non può verificarsi\n",
        "   - Quindi, $\\mathcal{A}$ influenza decisamente la probabilità di $\\mathcal{B}$\n",
        "\n",
        "### Esempio di Eventi Disgiunti\n",
        "1. **Lancio di un dado**\n",
        "   - Evento $\\mathcal{A}$: ottenere un numero pari\n",
        "   - Evento $\\mathcal{B}$: ottenere un numero dispari\n",
        "   - Sapendo che è uscito pari, la probabilità di dispari diventa zero\n",
        "\n",
        "## Esercizi\n",
        "\n",
        "1. **Determinare se i seguenti eventi sono indipendenti o dipendenti:**\n",
        "   - Ottenere testa in due lanci successivi di una moneta\n",
        "   - Pescare due carte rosse da un mazzo (senza rimettere la prima)\n",
        "   - La squadra di casa vince e piove durante la partita\n",
        "\n",
        "2. **Spiegare perché questi eventi sono disgiunti e quindi dipendenti:**\n",
        "   - Essere single e sposati contemporaneamente\n",
        "   - Trovarsi in due città diverse nello stesso momento\n",
        "   - Vincere e perdere la stessa partita\n",
        "\n",
        "\n",
        "> Ricorda: Due eventi $A$ e $B$ sono indipendenti se e solo se:\n",
        "\n",
        "  $$P(A \\cap B) = P(A) \\cdot P(B)$$\n",
        "\n",
        "### Proprietà fondamentali dell indipendenza\n",
        "\n",
        "1. Se $A$ e $B$ sono indipendenti, allora:\n",
        "   - $A$ e $B^c$ sono indipendenti\n",
        "   - $A^c$ e $B$ sono indipendenti\n",
        "   - $A^c$ e $B^c$ sono indipendenti\n",
        "\n",
        "2. La probabilità condizionata di eventi indipendenti rimane invariata:\n",
        "   $$P(A|B) = P(A) \\quad \\text{e} \\quad P(B|A) = P(B)$$\n",
        "\n",
        "3. La disgiunzione implica DIPENDENZA (o non indipendenza):\n",
        "   - Eventi disgiunti ($A \\cap B = \\emptyset$) con probabilità non nulla non possono essere indipendenti. Due eventi incompatibili (disgiunti) con probabilità non nulla sono sempre dipendenti.\n",
        "\n",
        "## Generalizzazione a più eventi\n",
        "\n",
        "Tre eventi $A$, $B$ e $C$ sono mutualmente indipendenti se:\n",
        "- $P(A \\cap B) = P(A) \\cdot P(B)$\n",
        "- $P(A \\cap C) = P(A) \\cdot P(C)$\n",
        "- $P(B \\cap C) = P(B) \\cdot P(C)$\n",
        "- $P(A \\cap B \\cap C) = P(A) \\cdot P(B) \\cdot P(C)$\n",
        "\n",
        "## Esempio\n",
        "\n",
        "Nel lancio di due dadi:\n",
        "- Sia $A$ l'evento \"il primo dado mostra 6\"\n",
        "- Sia $B$ l'evento \"il secondo dado mostra 6\"\n",
        "\n",
        "Questi eventi sono indipendenti poiché:\n",
        "- $P(A) = \\frac{1}{6}$\n",
        "- $P(B) = \\frac{1}{6}$\n",
        "- $P(A \\cap B) = \\frac{1}{36} = \\frac{1}{6} \\cdot \\frac{1}{6} = P(A) \\cdot P(B)$"
      ],
      "metadata": {
        "id": "lskrF9YWL7lu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# La Regola del Prodotto\n",
        "\n",
        "Per due eventi $\\mathcal{A}$ e $\\mathcal{B}$, la probabilità congiunta può essere espressa come:\n",
        "\n",
        "$$P(\\mathcal{A} \\cap \\mathcal{B}) = P(\\mathcal{B}) \\cdot P(\\mathcal{A}|\\mathcal{B}) = P(\\mathcal{A}) \\cdot P(\\mathcal{B}|\\mathcal{A})$$\n",
        "\n",
        "Per più eventi, questa regola si estende:\n",
        "\n",
        "$$P(\\mathcal{A} \\cap \\mathcal{B} \\cap \\mathcal{C}) = P(\\mathcal{A}) \\cdot P(\\mathcal{B}|\\mathcal{A}) \\cdot P(\\mathcal{C}|\\mathcal{A} \\cap \\mathcal{B})$$\n",
        "\n",
        "Questa scomposizione è particolarmente utile quando abbiamo una sequenza di eventi che seguono un ordine naturale o causale."
      ],
      "metadata": {
        "id": "qS-NCZY1tV2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Derivazione Intuitiva\n",
        "La regola del prodotto deriva direttamente dalla definizione di probabilità condizionata:\n",
        "\n",
        "$$P(\\mathcal{A}|\\mathcal{B}) = \\frac{P(\\mathcal{A} \\cap \\mathcal{B})}{P(\\mathcal{B})}$$\n",
        "\n",
        "Riorganizzando questa equazione, otteniamo:\n",
        "\n",
        "$$P(\\mathcal{A} \\cap \\mathcal{B}) = P(\\mathcal{B}) \\cdot P(\\mathcal{A}|\\mathcal{B})$$\n"
      ],
      "metadata": {
        "id": "papEZCObESUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decomposizioni alternative della Probabilità Congiunta\n",
        "Grazie alla proprietà commutativa dell'intersezione ($\\mathcal{A} \\cap \\mathcal{B} \\cap \\mathcal{C} = \\mathcal{B} \\cap \\mathcal{A} \\cap \\mathcal{C}$ etc.), possiamo decomporre la probabilità congiunta di tre eventi in diversi modi, tutti equivalenti:\n",
        "\n",
        "$$P(\\mathcal{A} \\cap \\mathcal{B} \\cap \\mathcal{C}) = P(\\mathcal{A}) \\cdot P(\\mathcal{B}|\\mathcal{A}) \\cdot P(\\mathcal{C}|\\mathcal{A} \\cap \\mathcal{B})$$ oppure:\n",
        "$$P(\\mathcal{A} \\cap \\mathcal{B} \\cap \\mathcal{C}) = P(\\mathcal{B}) \\cdot P(\\mathcal{A}|\\mathcal{B}) \\cdot P(\\mathcal{C}|\\mathcal{A} \\cap \\mathcal{B})$$\n",
        "o ancora:\n",
        "$$P(\\mathcal{A} \\cap \\mathcal{B} \\cap \\mathcal{C}) = P(\\mathcal{C}) \\cdot P(\\mathcal{A}|\\mathcal{C}) \\cdot P(\\mathcal{B}|\\mathcal{A} \\cap \\mathcal{C})$$\n",
        "\n",
        "E così via.\n",
        "\n",
        "Riesci a capire in quanti modi si può *fattorizzare* questa probabilità congiunta?\n",
        "\n",
        "Per calcolare il numero totale di decomposizioni possibili, osserviamo che:\n",
        "\n",
        "- Possiamo scegliere qualsiasi evento come primo termine (3 scelte)\n",
        "- Dei due eventi rimanenti, possiamo scegliere qualsiasi come secondo termine (2 scelte)\n",
        "- Così facendo, l'ultimo evento sarà automaticamente determinato (1 scelta)\n",
        "\n",
        "Per ogni sequenza di eventi, possiamo condizionare il secondo evento sul primo e il terzo sui primi due\n",
        "\n",
        "Quindi il numero totale di decomposizioni è:\n",
        "$3! = 6$ decomposizioni diverse\n",
        "\n",
        "Esplicitamente, sono:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "& P(\\mathcal{A}) \\cdot P(\\mathcal{B}|\\mathcal{A}) \\cdot P(\\mathcal{C}|\\mathcal{A} \\cap \\mathcal{B}) \\\\\n",
        "& P(\\mathcal{A}) \\cdot P(\\mathcal{C}|\\mathcal{A}) \\cdot P(\\mathcal{B}|\\mathcal{A} \\cap \\mathcal{C}) \\\\\n",
        "& P(\\mathcal{B}) \\cdot P(\\mathcal{A}|\\mathcal{B}) \\cdot P(\\mathcal{C}|\\mathcal{A} \\cap \\mathcal{B}) \\\\\n",
        "& P(\\mathcal{B}) \\cdot P(\\mathcal{C}|\\mathcal{B}) \\cdot P(\\mathcal{A}|\\mathcal{B} \\cap \\mathcal{C}) \\\\\n",
        "& P(\\mathcal{C}) \\cdot P(\\mathcal{A}|\\mathcal{C}) \\cdot P(\\mathcal{B}|\\mathcal{A} \\cap \\mathcal{C}) \\\\\n",
        "& P(\\mathcal{C}) \\cdot P(\\mathcal{B}|\\mathcal{C}) \\cdot P(\\mathcal{A}|\\mathcal{B} \\cap \\mathcal{C})\n",
        "\\end{align}$$\n",
        "\n",
        "\n",
        "Questa molteplicità di decomposizioni ci offre flessibilità nella risoluzione di problemi: possiamo scegliere la **decomposizione più conveniente in base alle informazioni disponibili nel problema specifico.**\n"
      ],
      "metadata": {
        "id": "HgCkqpNpDXbu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Esempi Pratici\n",
        "\n",
        "### 1. Estrazione di Carte\n",
        "Consideriamo l'estrazione di due carte da un mazzo standard di 52 carte senza reinserimento.\n",
        "\n",
        "Qual è la probabilità di estrarre due assi?\n",
        "\n",
        "1. Prima estrazione ($\\mathcal{A}_1$): $P(\\mathcal{A}_1) = \\frac{4}{52} = \\frac{1}{13}$\n",
        "2. Seconda estrazione ($\\mathcal{A}_2$), dato che abbiamo già estratto un asso:\n",
        "   $P(\\mathcal{A}_2|\\mathcal{A}_1) = \\frac{3}{51} = \\frac{1}{17}$\n",
        "\n",
        "La probabilità di estrarre due assi è quindi:\n",
        "\n",
        "$$P(\\mathcal{A}_1 \\cap \\mathcal{A}_2) = \\frac{1}{13} \\cdot \\frac{1}{17} = \\frac{1}{221}$$\n",
        "\n",
        "Si noti che questi eventi non sono indipendenti!\n",
        "\n",
        "### 2. Lancio di Dadi\n",
        "Lanciamo due dadi. Qual è la probabilità di ottenere un 6 sul primo dado e un numero pari sul secondo?\n",
        "\n",
        "1. Primo evento ($\\mathcal{D}_6$): ottenere 6 sul primo dado\n",
        "   $P(\\mathcal{D}_6) = \\frac{1}{6}$\n",
        "2. Secondo evento ($\\mathcal{P}$): ottenere un numero pari sul secondo dado\n",
        "   $P(\\mathcal{P}|\\mathcal{D}_6) = \\frac{3}{6} = \\frac{1}{2}$ (i due eventi sono indipendenti)\n",
        "\n",
        "$$P(\\mathcal{D}_6 \\cap \\mathcal{P}) = \\frac{1}{6} \\cdot \\frac{1}{2} = \\frac{1}{12}$$\n",
        "\n",
        "### 3. Esempio con Tre Eventi\n",
        "Consideriamo un processo di controllo qualità con tre fasi:\n",
        "\n",
        "1. $\\mathcal{M}$: il materiale passa il controllo iniziale\n",
        "2. $\\mathcal{D}$: il design è approvato\n",
        "3. $\\mathcal{T}$: il test finale è superato\n",
        "\n",
        "Supponiamo:\n",
        "- $P(\\mathcal{M}) = 0.9$ (90% dei materiali passa il controllo)\n",
        "- $P(\\mathcal{D}|\\mathcal{M}) = 0.8$ (80% dei design con materiali approvati passa)\n",
        "- $P(\\mathcal{T}|\\mathcal{M} \\cap \\mathcal{D}) = 0.95$ (95% dei prodotti con materiali e design approvati passa il test finale)\n",
        "\n",
        "La probabilità che un prodotto superi tutte e tre le fasi è:\n",
        "\n",
        "$$P(\\mathcal{M} \\cap \\mathcal{D} \\cap \\mathcal{T}) = 0.9 \\cdot 0.8 \\cdot 0.95 = 0.684$$\n",
        "\n",
        "### Casi Speciali\n",
        "\n",
        "#### Eventi Indipendenti\n",
        "Se gli eventi sono indipendenti, la regola del prodotto si semplifica:\n",
        "\n",
        "$$P(\\mathcal{A} \\cap \\mathcal{B}) = P(\\mathcal{A}) \\cdot P(\\mathcal{B})$$\n",
        "\n",
        "#### Eventi Mutuamente Esclusivi\n",
        "Se gli eventi sono mutuamente esclusivi e non vuoti:\n",
        "\n",
        "$$P(\\mathcal{A} \\cap \\mathcal{B}) = 0$$\n",
        "\n",
        "## Applicazioni\n",
        "\n",
        "La regola del prodotto è fondamentale in molti campi:\n",
        "\n",
        "1. **Analisi dei Rischi**: Calcolare la probabilità di una serie di eventi che devono verificarsi tutti per un fallimento del sistema\n",
        "2. **Machine Learning**: Nella costruzione di modelli probabilistici, come le catene di Markov, tanto usate per il decision making sequenziale e la massimizzazione di profitto a lungo periodo.\n",
        "3. **Genetica**: Calcolare la probabilità di ereditare specifiche combinazioni di geni.\n",
        "\n",
        "## Visualizzazione con Alberi delle Probabilità\n",
        "\n",
        "Gli alberi delle probabilità sono strumenti utili per visualizzare e calcolare probabilità usando la regola del prodotto:\n",
        "\n",
        "- nei nodi andiamo a collocare gli eventi. Iniziando col mettere al nodo radice, l'evento certo, cioè $\\Omega$.\n",
        "- Ogni nodo genera dei nodi figli, che definiscono una partizione sul nodo precedente, cioè una serie di tutti i casi possibili e mutuamente esclusivi che si possono dare dopo che si è dato l'evento padre. Nel più comune dei casi, ogni nodo $\\mathcal{A}$ genera due figli, quello relazionato con un possibile evento successivo $\\mathcal{B}$ e il suo complementare $\\bar{\\mathcal{B}}$. (Si noti che, per qualiasi evento $\\mathcal{B}$, si ha che  $\\{\\mathcal{B},\\bar{\\mathcal{B}}\\}$ è una partizione). Ai nodi ci possono essere eventi singoli come $\\mathcal{A}$ oppure eventi congiunti come $\\mathcal{A} \\cap \\mathcal{B}$.\n",
        "- Nei rami andiamo a scrivere le probabilità condizionate a seconda dell'evento congiunto che il ramo genera. Vediamo un esempio:\n",
        "\n",
        "```\n",
        "                            P(A∩B) = 0.28\n",
        "                            /\n",
        "                           /\n",
        "                      P(B|A) = 0.7\n",
        "                         /                     \n",
        "              P(A) = 0.4                      \n",
        "            /           \\                     \n",
        "           /          P(B'|A) = 0.3\n",
        "P(Ω)=1.0 ---              \\\n",
        "           \\               \\\n",
        "            \\               P(A∩B') = 0.12\n",
        "             \\            \n",
        "              \\           \n",
        "               \\            P(A'∩B) = 0.18\n",
        "                \\          /\n",
        "                 \\        /\n",
        "                  \\    P(B|A') = 0.3\n",
        "                   \\    /\n",
        "              P(A') = 0.6\n",
        "                        \\\n",
        "                        P(B'|A') = 0.7\n",
        "                          \\           \n",
        "                           \\              \n",
        "                            P(A'∩B') = 0.42        \n",
        "```\n",
        "\n",
        "- Questo tipo di visualizzazione aiuta a capire come, per arrivare a calcolare le probabilità degli eventi sui nodi, possiamo semplicemente molteplicare le probabilità si moltiplicano lungo i rami dell'albero.\n",
        "\n",
        "- Se non resta chiaro perché abbiamo usato $P(\\mathcal{A})$, che non è sui rami dell'albero, allora potremmo disegnare diversamente l'albero:\n",
        "\n",
        "\n",
        "```\n",
        "                            P(A∩B) = 0.28\n",
        "                           /\n",
        "                          /\n",
        "                    P(B|A) = 0.7\n",
        "                        /                     \n",
        "                       A\n",
        "                     /  \\\n",
        "                    /   P(B'|A) = 0.3\n",
        "          P(A|Ω) = 0.4    \\                 \n",
        "                 /         \\                     \n",
        "                /          P(A∩B') = 0.12\n",
        "P(Ω)=1.0 -------            \n",
        "                \\                   \n",
        "                 \\            \n",
        "              P(A'|Ω) = 0.6           \n",
        "                   \\          P(A'∩B) = 0.18\n",
        "                    \\        /\n",
        "                     \\      /\n",
        "                      \\  P(B|A') = 0.3\n",
        "                       \\  /\n",
        "                        A'\n",
        "                          \\\n",
        "                        P(B'|A') = 0.7\n",
        "                             \\           \n",
        "                              \\              \n",
        "                             P(A'∩B') = 0.42        \n",
        "```\n",
        "E ora dovrebbe essere più chiaro come, per arrivare alle probabilità degli eventi sui nodi, si moltiplicano le probabilità sui rami."
      ],
      "metadata": {
        "id": "JORViGQAugmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nei seguenti diagrammi, per comodità, le probabilità congiunte del tipo $P(\\mathcal{A}\\cap \\mathcal{B})$, vengono espresse così: $ P(\\mathcal{A} , \\mathcal{B})$, oppure direttamente così $ \\mathcal{A} , \\mathcal{B}$:\n",
        "\n",
        "\n",
        "### Esempio 1: Test Medico\n",
        "\n",
        "Consideriamo un test medico per una malattia rara:\n",
        "- Prevalenza della malattia: 1%\n",
        "- Sensibilità del test: 95% (P(Positivo|Malattia))\n",
        "- Specificità del test: 90% (P(Negativo|No Malattia))\n",
        "\n",
        "```                \n",
        "                         P(M∩+) = 0.0095\n",
        "                        /\n",
        "                       /\n",
        "                      P(+|M) = 0.95\n",
        "                     /\n",
        "            P(M) = 0.01\n",
        "           /         \\\n",
        "          /           P(-|M) = 0.05\n",
        "P(Ω) = 1.0             \\\n",
        "         \\              P(M∩-) = 0.0005\n",
        "          \\\n",
        "           \\                P(NM∩+) = 0.099\n",
        "            \\             /\n",
        "             \\         P(+|NM) = 0.10\n",
        "              \\         /\n",
        "              P(NM) = 0.99\n",
        "                        \\\n",
        "                        P(-|NM) = 0.90\n",
        "                          \\\n",
        "                           P(NM∩-) = 0.891\n",
        "```\n",
        "\n",
        "Calcoli utili:\n",
        "- P(Test Positivo) = 0.0095 + 0.099 = 0.1085\n",
        "- P(Malattia|Test Positivo) = 0.0095 / 0.1085 ≈ 0.088  (Vedremo tra poco come abbiamo calcolato questo caso, ma per esercizio, potresti cercare di arrivarci giocando con la definizione e le proprietà di probabilità congiunta e condizionata ;)\n",
        "\n",
        "\n",
        "\n",
        "## Esempio 2: Tre Lanci di Moneta\n",
        "\n",
        "Probabilità di ottenere testa $P(T) = 0.5$, prob. di ottenere croce $P(C) = 0.5$. Notiamo che l'esito dei lanci successivi non viene influenzato da quello dei lanci precedenti, sono eventi indipendenti infatti.\n",
        "\n",
        "Si noti anche come è naturale scrivere o leggere le ramificazione dell'albero seguendo la sucessione temporale dei lanci di moneta.\n",
        "\n",
        "In questo e nei prossimi diagrammi ommettiamo le probabilità condizionate sui rami, per semplicità di disegno. Ad ogni modo si noti che le probabilità condizionate qui sono uguali a quelle \"marginali\" per l'ipotesi di indipendenza tra eventi:\n",
        "\n",
        "$P(T|C)= P(T)$ e $P(T|T \\cap C)=P(T)$, ecc...\n",
        "\n",
        "\n",
        "```\n",
        "                                            P(T∩T∩T) = 0.125\n",
        "                                           /\n",
        "                                          /\n",
        "                            P(T∩T) = 0.25\n",
        "                           /              \\\n",
        "                          /                \\\n",
        "                P(T) = 0.5                  P(T∩T∩C) = 0.125\n",
        "               /          \\\n",
        "              /            \\                P(T∩C∩T) = 0.125\n",
        "             /              \\              /\n",
        "P(Ω) = 1.0                    P(T∩C) = 0.25            \n",
        "             \\                             \\\n",
        "              \\                             P(T∩C∩C) = 0.125\n",
        "               \\        \n",
        "                \\\n",
        "                 \\\n",
        "                  \\                            P(C∩T∩T) = 0.125\n",
        "                   \\                          /\n",
        "                    \\            P(C∩T) = 0.25\n",
        "                     \\          /             \\\n",
        "                      P(C) = 0.5               P(C∩T∩C) = 0.125\n",
        "                                \\           \n",
        "                                 \\\n",
        "                                  \\               P(C∩C∩T) = 0.125\n",
        "                                   \\             /\n",
        "                                    P(C∩C) = 0.25\n",
        "                                                 \\\n",
        "                                                  P(C∩C∩C) = 0.125\n",
        "```\n",
        "\n",
        "## Esempio 3: Processo di Controllo Qualità\n",
        "\n",
        "Un prodotto passa attraverso tre controlli:\n",
        "1. Controllo materiali (M): 90% passa\n",
        "2. Controllo design (D): 80% passa se i materiali sono ok, 40% altrimenti\n",
        "3. Test finale (T): 95% passa se tutto ok, 60% se fallito un controllo, 30% se falliti entrambi\n",
        "\n",
        "```\n",
        "                                            P(M∩D∩T) = 0.684\n",
        "                                           /\n",
        "                                          /\n",
        "                            P(M∩D) = 0.72\n",
        "                           /              \\\n",
        "                          /                \\\n",
        "                P(M) = 0.9                  P(M∩D∩T') = 0.036\n",
        "               /          \\\n",
        "              /            \\                P(M∩D'∩T) = 0.108\n",
        "             /              \\              /\n",
        "P(Ω) = 1.0                    P(M∩D') = 0.18            \n",
        "             \\                             \\\n",
        "              \\                             P(M∩D'∩T') = 0.072\n",
        "               \\        \n",
        "                \\\n",
        "                 \\\n",
        "                  \\                            P(M'∩D∩T) = 0.024\n",
        "                   \\                          /\n",
        "                    \\            P(M'∩D) = 0.04\n",
        "                     \\          /             \\\n",
        "                      P(M') = 0.1              P(M'∩D∩T') = 0.016\n",
        "                                \\           \n",
        "                                 \\\n",
        "                                  \\               P(M'∩D'∩T) = 0.018\n",
        "                                   \\             /\n",
        "                                    P(M'∩D') = 0.06\n",
        "                                                 \\\n",
        "                                                  P(M'∩D'∩T') = 0.042\n",
        "```\n",
        "\n",
        "Dove:\n",
        "- M' significa \"fallito controllo materiali\"\n",
        "- D' significa \"fallito controllo design\"\n",
        "- T' significa \"fallito test finale\"\n",
        "\n",
        "## Utilizzo degli Alberi delle Probabilità\n",
        "\n",
        "Gli alberi sono particolarmente utili per:\n",
        "\n",
        "1. **Visualizzare sequenze di eventi**\n",
        "   - Mostrare chiaramente come gli eventi si susseguono\n",
        "\n",
        "2. **Calcolare probabilità congiunte**\n",
        "   - Moltiplicare le probabilità lungo i rami (nei casi di sopra non abbiamo scritto le probabilità lungo i rami, ma abbiamo assunto di avere già le probabilità congiunte, che sono le probabilità degli eventi che ogni nodo indica.\n",
        "\n",
        "3. **Trovare probabilità marginali**\n",
        "   - Sommare le probabilità dei rami pertinenti\n",
        "\n",
        "4. **Applicare il teorema di Bayes**! (vedremo cosa sia)\n",
        "   - Visualizzare le relazioni tra probabilità \"a priori\" e condizionate\n",
        "\n",
        "### Suggerimenti per gli esercizi\n",
        "\n",
        "1. **Disegna l'albero completo**\n",
        "   - Non saltare passaggi, anche se sembrano ovvi\n",
        "\n",
        "2. **Scrivi tutte le probabilità**\n",
        "   - Lungo i rami le probabilità condizionate\n",
        "   - Volendo, ai nodi le probabilità congiunte\n",
        "\n",
        "3. **Verifica i totali**\n",
        "   - La somma delle probabilità sugli eventi ad ogni ramificazione deve essere 1\n"
      ],
      "metadata": {
        "id": "bgwCLMOryJFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Il Teorema della Probabilità Totale\n",
        "\n",
        "### Concetto di Partizione\n",
        "\n",
        "Una partizione dello spazio campionario $\\Omega$ è un insieme di eventi $\\{\\mathcal{B}_1, \\mathcal{B}_2, ..., \\mathcal{B}_n\\}$ tali che:\n",
        "1. Sono mutuamente esclusivi: $\\mathcal{B}_i \\cap \\mathcal{B}_j = \\emptyset$ per $i \\neq j$\n",
        "2. Sono esaustivi: $\\bigcup_{i=1}^n \\mathcal{B}_i = \\Omega$\n",
        "3. Sono non vuoti: $P(\\mathcal{B}_i) > 0$ per ogni $i$\n",
        "\n",
        "### Teorema\n",
        "\n",
        "Per qualsiasi evento $\\mathcal{A}$ e una partizione $\\{\\mathcal{B}_1, \\mathcal{B}_2, ..., \\mathcal{B}_n\\}$:\n",
        "\n",
        "$$P(\\mathcal{A}) = \\sum_{i=1}^n P(\\mathcal{A}|\\mathcal{B}_i) \\cdot P(\\mathcal{B}_i)$$\n",
        "\n",
        "Questo teorema ci permette di calcolare la probabilità di un evento scomponendola attraverso una partizione dello spazio campionario."
      ],
      "metadata": {
        "id": "CtiIrwHztfES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Il Teorema di Bayes e l'inferenza"
      ],
      "metadata": {
        "id": "lk27R9bi0Y5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "L'inferenza è il processo di trarre conclusioni partendo da premesse o evidenze. Nel contesto probabilistico, spesso vogliamo:\n",
        "1. Capire la causa di un effetto osservato\n",
        "2. Prevedere effetti futuri basandoci su cause note\n",
        "\n",
        "La probabilità condizionata è fondamentale in questo processo perché ci permette di aggiornare le nostre credenze alla luce di nuove informazioni."
      ],
      "metadata": {
        "id": "sB3jzRqntwpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### Esempio: Diagnosi Medica\n",
        "- Causa ($\\mathcal{D}$): presenza di una malattia\n",
        "- Effetto ($\\mathcal{S}$): presenza di un sintomo\n",
        "\n",
        "Conosciamo:\n",
        "- $P(\\mathcal{D})$: probabilità a priori della malattia\n",
        "- $P(\\mathcal{S}|\\mathcal{D})$: probabilità del sintomo data la malattia (verosimiglianza)\n",
        "\n",
        "Vogliamo:\n",
        "- $P(\\mathcal{D}|\\mathcal{S})$: probabilità della malattia dato il sintomo (probabilità a posteriori)\n",
        "\n",
        "\n",
        "\n",
        "### Componenti Chiave\n",
        "1. **Probabilità a priori** $P(\\mathcal{H})$: la nostra credenza iniziale su un'ipotesi\n",
        "2. **Verosimiglianza** $P(\\mathcal{E}|\\mathcal{H})$: probabilità dell'evidenza data l'ipotesi\n",
        "3. **Probabilità a posteriori** $P(\\mathcal{H}|\\mathcal{E})$: la credenza aggiornata dopo aver osservato l'evidenza\n",
        "\n",
        "### Formula del teorema di Bayes\n",
        "\n",
        "$$P(\\mathcal{H}|\\mathcal{E}) = \\frac{P(\\mathcal{E}|\\mathcal{H}) \\cdot P(\\mathcal{H})}{P(\\mathcal{E})}$$\n",
        "\n",
        "dove $P(\\mathcal{E})$ può essere calcolato usando il teorema della probabilità totale:\n",
        "\n",
        "$$P(\\mathcal{E}) = P(\\mathcal{E}|\\mathcal{H}) \\cdot P(\\mathcal{H}) + P(\\mathcal{E}|\\neg\\mathcal{H}) \\cdot P(\\neg\\mathcal{H})$$\n",
        "\n",
        "### Interpretazione\n",
        "Il teorema di Bayes ci fornisce un metodo formale per:\n",
        "1. Aggiornare le nostre credenze alla luce di nuove evidenze\n",
        "2. Invertire la direzione della probabilità condizionata (da effetto a causa)\n",
        "3. Combinare informazioni a priori con nuovi dati\n"
      ],
      "metadata": {
        "id": "8Z7lDLgJtMgt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Esempio Numerico\n",
        "Consideriamo un test medico:\n",
        "- $P(\\mathcal{D}) = 0.01$ (1% della popolazione ha la malattia)\n",
        "- $P(\\mathcal{S}|\\mathcal{D}) = 0.95$ (95% di sensibilità del test)\n",
        "- $P(\\mathcal{S}|\\neg\\mathcal{D}) = 0.10$ (10% di falsi positivi)\n",
        "\n",
        "Usando Bayes:\n",
        "\n",
        "$$P(\\mathcal{D}|\\mathcal{S}) = \\frac{0.95 \\cdot 0.01}{0.95 \\cdot 0.01 + 0.10 \\cdot 0.99} \\approx 0.088$$\n",
        "\n",
        "Quindi, anche con un test positivo, la probabilità di avere la malattia è solo circa 8.8%."
      ],
      "metadata": {
        "id": "FR--7etAt6Mq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Il Teorema di Bayes nell'Era dell'Intelligenza Artificiale Moderna\n",
        "\n",
        "Il teorema di Bayes, formulato più di 250 anni fa, si è rivelato sorprendentemente centrale nello sviluppo dell'intelligenza artificiale moderna. La sua importanza va ben oltre la semplice statistica, permeando sia la nostra comprensione del cervello umano che lo sviluppo di potenti algoritmi di machine learning.\n",
        "\n",
        "Nel campo delle neuroscienze cognitive, l'ipotesi del \"cervello bayesiano\" suggerisce che il nostro cervello funzioni essenzialmente come una macchina di inferenza probabilistica. Secondo questa teoria, percepiamo il mondo non semplicemente raccogliendo informazioni sensoriali, ma piuttosto combinando continuamente le nostre aspettative precedenti (prior) con nuove evidenze (likelihood) per aggiornare la nostra comprensione del mondo (posterior). Questo processo di aggiornamento continuo delle nostre \"credenze\" sul mondo è essenzialmente un'implementazione biologica del teorema di Bayes.\n",
        "\n",
        "Parallelamente, nel campo dell'intelligenza artificiale, l'inferenza bayesiana si è rivelata fondamentale per lo sviluppo di modelli generativi avanzati. Tuttavia, calcolare esattamente le probabilità posteriori in spazi ad alta dimensionalità - come quelli necessari per generare immagini, video o testo - è computazionalmente intrattabile. La svolta è arrivata con lo sviluppo di metodi di approssimazione dell'inferenza bayesiana, in particolare l'inferenza variazionale. Questi metodi permettono di approssimare distribuzioni di probabilità complesse con altre più semplici e trattabili.\n",
        "\n",
        "Questa intuizione ha portato allo sviluppo dei Variational Autoencoders (VAE) e ha influenzato profondamente l'architettura di molti modelli generativi moderni. Anche se modelli come i Diffusion Models o le moderne architetture Transformer non sono esplicitamente bayesiani, incorporano spesso principi di ragionamento probabilistico ispirati all'inferenza bayesiana. Per esempio, il processo di denoising nei modelli di diffusione può essere interpretato come una forma di inferenza bayesiana, dove il modello gradualmente \"raffina\" la sua stima di un'immagine o un video partendo da puro rumore.\n",
        "\n",
        "L'impatto di questo approccio probabilistico si estende alla robustezza dei modelli di AI. Incorporando l'incertezza nei loro predizioni e decisioni, i sistemi di AI moderni possono gestire meglio situazioni ambigue o dati rumorosi. Inoltre, la capacità di quantificare l'incertezza è cruciale in applicazioni critiche come la diagnosi medica o la guida autonoma.\n",
        "\n",
        "Guardando al futuro, mentre continuiamo a sviluppare modelli di AI sempre più sofisticati, i principi dell'inferenza bayesiana rimarranno probabilmente centrali. Non solo come strumento matematico, ma come framework concettuale per pensare all'apprendimento, alla percezione e alla decisione, sia nelle macchine che nel cervello umano. La sfida continua sarà sviluppare metodi sempre più efficienti per approssimare l'inferenza bayesiana in spazi di alta dimensionalità, permettendo ai nostri modelli di ragionare in modo sempre più sofisticato sull'incertezza."
      ],
      "metadata": {
        "id": "reYFdjlt2WBY"
      }
    }
  ]
}