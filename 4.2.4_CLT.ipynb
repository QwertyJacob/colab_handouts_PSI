{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "887ac92c",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/QwertyJacob/colab_handouts_PSI/blob/main/4.2..4_CLT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a058e041",
   "metadata": {},
   "source": [
    "## 4.3 Distribuzioni Congiunte:\n",
    "__________________\n",
    "Addattamento da: \n",
    "- Probability and Statistics for Computer Scientists, M. Baron, CRC Press, 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331b3c27",
   "metadata": {},
   "source": [
    "Spesso dovremmo considerare diverse variabili aleatorie simultaneamente. Possiamo per esempio dover pensare alla dimensione di una RAM e la velocità di una CPU, il prezzo di un computer e la sua capacità, la temperatura e l'umidità, le prestazioni tecniche e artistiche, ecc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b266a5",
   "metadata": {},
   "source": [
    "\n",
    "### 4.3.1 Distribuzione congiunta e distribuzioni marginali\n",
    "\n",
    "> **DEFINIZIONE 4.4**  **Distribuzione congiunta**. Se $ X $ e $ Y $ sono variabili aleatorie, allora la coppia $ (X, Y) $ è un **vettore aleatorio**. La sua distribuzione è chiamata **distribuzione congiunta** di $ X $ e $ Y $. Le distribuzioni individuali di $ X $ e $ Y $ sono quindi chiamate distribuzioni **marginali**.\n",
    "\n",
    "Anche se in questa sezione parliamo di due variabili aleatorie, tutti i concetti si estendono a un vettore $(X_1, X_2, \\ldots, X_n)$ di $ n $ componenti e alla sua distribuzione congiunta.\n",
    "\n",
    "In modo analogo al caso di una singola variabile, la distribuzione congiunta di un vettore è una collezione di probabilità che, per esempio, due variabili aleatorie $(X, Y)$ assumano, rispettivamente, un valore $(x, y)$. Quando diciamo che $X=x$ \"e\" $Y=y$, questo \"e\" indica l'intersezione, quindi la funzione di massa di probabilità congiunta di $ X $ e $ Y $. In linguaggio matematico si scrive:\n",
    "$$\n",
    "P(x, y) = P((X, Y) = (x, y)) = P(X = x \\cap Y = y).\n",
    "$$\n",
    "Ancora una volta, tutti gli eventi possibili  $ \\{(X, Y) = (x, y)\\} $ sono esaustivi e mutuamente esclusivi per coppie diverse $ (x, y) $, quindi\n",
    "$$\n",
    "\\sum_x \\sum_y P(x, y) = 1.\n",
    "$$\n",
    "\n",
    "La distribuzione congiunta di $ (X, Y) $ contiene informazioni complete sul comportamento di questo vettore aleatorio. In particolare, le funzioni di massa di probabilità marginali di $ X $ e $ Y $ possono essere ottenute dalla funzione di massa congiunta mediante la regola dell'addizione. (Figura 3.2)\n",
    "\n",
    "![Figure 3.2](figs/3.2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fc4ebf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "# Parametri della simulazione  \n",
    "num_walkers = 10000  # Numero di \"camminatori\" (particelle o simulazioni indipendenti)  \n",
    "num_steps = 1000     # Numero di passi per ciascun camminatore  \n",
    "\n",
    "# Simula i passi casuali: +1 o -1 con probabilità uguale  \n",
    "steps = np.random.choice([-1, 1], size=(num_walkers, num_steps))  \n",
    "\n",
    "# Calcola la posizione finale per ciascun camminatore (somma dei passi)  \n",
    "positions = np.sum(steps, axis=1)  \n",
    "\n",
    "# Plot dell'istogramma delle posizioni finali (normalizzato per ottenere una densità di probabilità)  \n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.hist(positions, bins=50, density=True, alpha=0.6, color='green', label='Istogramma delle posizioni')  \n",
    "\n",
    "# Calcola media e deviazione standard (dovrebbero essere circa 0 e sqrt(num_steps))  \n",
    "mu = np.mean(positions)  \n",
    "sigma = np.std(positions)  \n",
    "\n",
    "# Sovraimponi la curva gaussiana teorica  \n",
    "x = np.linspace(mu - 4*sigma, mu + 4*sigma, 100)  \n",
    "gaussian = (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(- (x - mu)**2 / (2 * sigma**2))  \n",
    "plt.plot(x, gaussian, 'r--', linewidth=2, label='Distribuzione Gaussiana')  \n",
    "\n",
    "# Aggiungi titoli e etichette  \n",
    "plt.title('Simulazione di Random Walk: Evoluzione verso Distribuzione Gaussiana')  \n",
    "plt.xlabel('Posizione Finale')  \n",
    "plt.ylabel('Densità di Probabilità')  \n",
    "plt.legend()  \n",
    "plt.grid(True)\n",
    "# Se vuoi salvare il grafico: \n",
    "# plt.savefig('random_walk.png')\n",
    "plt.show()  \n",
    "\n",
    "# Stampa alcune statistiche per verifica  \n",
    "print(f\"Media delle posizioni: {mu:.2f} (attesa ~0)\")  \n",
    "print(f\"Deviazione standard: {sigma:.2f} (attesa ~sqrt({num_steps}) = {np.sqrt(num_steps):.2f})\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
