{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e948a135",
      "metadata": {
        "id": "e948a135"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QwertyJacob/colab_handouts_PSI/blob/main/BONUSX_ActiveInference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f4fb729",
      "metadata": {
        "id": "4f4fb729"
      },
      "source": [
        "# A hopefully simple tutorial on Active inference for continuous PO-MDPs\n",
        "_______\n",
        "By Jesus Cevallos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae3da76a",
      "metadata": {
        "id": "ae3da76a"
      },
      "source": [
        "## Markov Decision Processes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11a80371",
      "metadata": {
        "id": "11a80371"
      },
      "source": [
        "### Why we need Markov Decision Processes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "513f7d78",
      "metadata": {
        "id": "513f7d78"
      },
      "source": [
        "\n",
        "Imagine an agent — a robot, an animal, or even a thermostat — that interacts with an environment over time.  \n",
        "At each moment:\n",
        "\n",
        "- The environment has **some state**, describing what is going on.\n",
        "- The agent takes **an action**.\n",
        "- The environment changes in response.\n",
        "- The agent receives **feedback** (in reinforcement learning) or *perceptions* (in active inference).\n",
        "\n",
        "To reason about such interactions, we need a mathematical model that:\n",
        "\n",
        "- Represents how the world evolves over time  \n",
        "- Represents how the agent influences the world  \n",
        "- Allows us to compute or infer how good (or *expected*) different action sequences are  \n",
        "\n",
        "The simplest such model is a **Markov Decision Process (MDP)**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "771e7e0a",
      "metadata": {
        "id": "771e7e0a"
      },
      "source": [
        "### What is a Markov Decision Process?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b978ff8a",
      "metadata": {
        "id": "b978ff8a"
      },
      "source": [
        "An MDP is defined by:\n",
        "\n",
        "$$\n",
        "\\mathcal{M} = (\\mathcal{S},\\mathcal{A},P(s_{t+1}\\mid s_t, a_t), R(s_t,a_t))\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- **$\\mathcal{S}$** is the set of possible *states* of the world  \n",
        "- **$\\mathcal{A}$** is the set of possible *actions* the agent can take  \n",
        "- **$P(s_{t+1} \\mid s_t, a_t)$** is the probability of the next state  \n",
        "- **$R(s_t,a_t)$** (only in RL) is a reward function  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4028fb1",
      "metadata": {
        "id": "a4028fb1"
      },
      "source": [
        "### The Markov Property\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6815c72",
      "metadata": {
        "id": "f6815c72"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "The key assumption that makes MDPs simple and powerful is:\n",
        "\n",
        "$$\n",
        "\\text{The future depends only on the present, not on the whole past.}\n",
        "$$\n",
        "\n",
        "Formally:\n",
        "\n",
        "$$\n",
        "P(s_{t+1} \\mid s_{0:t}, a_{0:t}) = P(s_{t+1} \\mid s_t, a_t).\n",
        "$$\n",
        "\n",
        "This is a *memorylessness* assumption.  \n",
        "The idea is: **the present state summarizes all relevant information about the past**.\n",
        "\n",
        "This dramatically simplifies computations.\n",
        "\n",
        "Without the Markov assumption, state prediction would require the entire history, which is computationally explosive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a17dc90",
      "metadata": {
        "id": "9a17dc90"
      },
      "source": [
        "### The Graphical (Causal) Structure of an MDP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b45177fc",
      "metadata": {
        "id": "b45177fc"
      },
      "source": [
        "\n",
        "An MDP can be expressed as a causal diagram (run cell bellow):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ff9e2d6",
      "metadata": {
        "id": "2ff9e2d6",
        "outputId": "35307a51-c2ee-42fa-8340-fcb748eb8d7c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "G = nx.DiGraph()\n",
        "\n",
        "G.add_nodes_from([\"s_t\", \"a_t\", \"s_{t+1}\"])\n",
        "\n",
        "G.add_edges_from([\n",
        "    (\"s_t\", \"a_t\"),        # Policy depends on state\n",
        "    (\"s_t\", \"s_{t+1}\"),    # Transition depends on state\n",
        "    (\"a_t\", \"s_{t+1}\")     # Transition also depends on action\n",
        "])\n",
        "\n",
        "pos = {\n",
        "    \"s_t\": (0, 1),\n",
        "    \"a_t\": (0, 0),\n",
        "    \"s_{t+1}\": (2, 1)\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(5,3))\n",
        "nx.draw(G, pos, with_labels=True, node_color=\"lightgray\", node_size=3000)\n",
        "plt.title(\"MDP Causal Graph\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ca6ab23",
      "metadata": {
        "id": "2ca6ab23"
      },
      "source": [
        "Where the arrows mean “causes” or “influences”.\n",
        "\n",
        "More explicitly:\n",
        "\n",
        "- The state at time $t$ causes the next state  \n",
        "- The action also contributes causally  \n",
        "\n",
        "MDPs are called **fully observable** because states are supposed to be visible."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c734f417",
      "metadata": {
        "id": "c734f417"
      },
      "source": [
        "### Why MDPs Are Not Enough\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7be2eaaa",
      "metadata": {
        "id": "7be2eaaa"
      },
      "source": [
        "\n",
        "MDPs assume the agent *sees* the world’s true state $s_t$.\n",
        "\n",
        "But in real life:\n",
        "\n",
        "* A robot only sees sensor readings\n",
        "* A human only perceives noisy sensory data\n",
        "* A self-driving car sees camera images, not true distances\n",
        "* A rat navigating a maze does not know which room it is in with certainty\n",
        "\n",
        "In all realistic situations, the agent receives **uncertain** or **indirect** information about the true state.\n",
        "\n",
        "This motivates **Partially Observed MDPs (POMDPs)**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e894dea",
      "metadata": {
        "id": "9e894dea"
      },
      "source": [
        "### From MDPs to POMDPs: The Need for Hidden States\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9083fdf6",
      "metadata": {
        "id": "9083fdf6"
      },
      "source": [
        "\n",
        "In a POMDP, the agent receives **observations**:\n",
        "\n",
        "* $o_t$ is what the agent *observes*\n",
        "* $s_t$ is the true (hidden) state\n",
        "* There is a mapping (likelihood model)\n",
        "  $$\n",
        "  P(o_t \\mid s_t)\n",
        "  $$\n",
        "\n",
        "This makes the problem **much more realistic**, because it models:\n",
        "\n",
        "* sensor noise\n",
        "* perceptual ambiguity\n",
        "* incomplete information\n",
        "* uncertainty about what is happening\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19ece2b7",
      "metadata": {
        "id": "19ece2b7"
      },
      "source": [
        "### Formal Structure of a POMDP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abf93baa",
      "metadata": {
        "id": "abf93baa"
      },
      "source": [
        "\n",
        "A POMDP is:\n",
        "\n",
        "$$\n",
        "\\mathcal{P} = (\\mathcal{S}, \\mathcal{A}, \\mathcal{O},\n",
        "P(s_{t+1}\\mid s_t,a_t), P(o_t\\mid s_t))\n",
        "$$\n",
        "\n",
        "Differences from MDP:\n",
        "\n",
        "* Observations $o_t$ are noisy functions of hidden states\n",
        "* Agent must **infer** states from observations\n",
        "* This leads to beliefs, filtering, and Bayesian inference\n",
        "\n",
        "In other words, we go from:\n",
        "\n",
        "MDP:\n",
        "**State is known → choose action**\n",
        "\n",
        "POMDP:\n",
        "**State is unknown → infer hidden state → choose action**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c9e8438",
      "metadata": {
        "id": "5c9e8438"
      },
      "source": [
        "### The Causal Graph of a POMDP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2989957e",
      "metadata": {
        "id": "2989957e"
      },
      "source": [
        "\n",
        "A POMDP can be visualized running the cell below:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ec68d5b",
      "metadata": {
        "id": "6ec68d5b"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9612240d",
      "metadata": {
        "id": "9612240d",
        "outputId": "4e93faf0-59b6-4300-ae6c-9620064757f7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 700x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "G = nx.DiGraph()\n",
        "\n",
        "nodes = [\"s_t\", \"a_t\", \"s_{t+1}\", \"o_t\", \"o_{t+1}\"]\n",
        "G.add_nodes_from(nodes)\n",
        "\n",
        "edges = [\n",
        "    (\"s_t\", \"a_t\"),\n",
        "    (\"s_t\", \"o_t\"),\n",
        "    (\"s_t\", \"s_{t+1}\"),\n",
        "    (\"a_t\", \"s_{t+1}\"),\n",
        "    (\"s_{t+1}\", \"o_{t+1}\")\n",
        "]\n",
        "\n",
        "G.add_edges_from(edges)\n",
        "\n",
        "pos = {\n",
        "    \"s_t\": (0, 1),\n",
        "    \"a_t\": (0, 0),\n",
        "    \"o_t\": (0, 2),\n",
        "    \"s_{t+1}\": (2, 1),\n",
        "    \"o_{t+1}\": (2, 2)\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "nx.draw(G, pos, with_labels=True, node_color=\"lightgray\", node_size=3000)\n",
        "plt.title(\"POMDP Causal Graph\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "332edb9d",
      "metadata": {
        "id": "332edb9d"
      },
      "source": [
        "* Hidden states cause observations\n",
        "* Hidden states evolve based on actions\n",
        "\n",
        "This structure is **exactly** the generative model used in active inference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a8796c6",
      "metadata": {
        "id": "1a8796c6"
      },
      "source": [
        "### Conditional Independences in POMDPs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b483e2a2",
      "metadata": {
        "id": "b483e2a2"
      },
      "source": [
        "\n",
        "\n",
        "The heart of the POMDP structure is the **conditional independence** that hidden states enforce:\n",
        "\n",
        "#### Observations depend only on the current hidden state\n",
        "\n",
        "$$\n",
        " P(o_t \\mid s_t, a_{t-1}, s_{t-1}, o_{t-1}, \\ldots) = P(o_t \\mid s_t)\n",
        "$$\n",
        "\n",
        "Meaning:\n",
        "\n",
        "> Once you know the current hidden state, the observation no longer depends on the past. (Note that observations are also independent of previous actions given states...)\n",
        "\n",
        "This is a **causal shielding** property.\n",
        "\n",
        "#### State transitions depend only on current state and action\n",
        "\n",
        "$$\n",
        "P(s_{t+1} \\mid s_t, a_t, s_{t-1}, o_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)\n",
        "$$\n",
        "\n",
        "The hidden state encodes all relevant information for prediction. (We do not care about previous observations either)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0b6e26d",
      "metadata": {
        "id": "f0b6e26d"
      },
      "source": [
        "## The Generative Process vs. The Generative Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fb53336",
      "metadata": {
        "id": "9fb53336"
      },
      "source": [
        "\n",
        "Up to this point, we've described the **generative process**: the actual, external dynamics of the world as modeled by a POMDP. This is the \"true\" mechanism by which hidden states evolve, actions influence those states, and observations are generated. In the generative process, we use probabilities denoted by $P$, such as $P(s_{t+1} \\mid s_t, a_t)$ for state transitions and $P(o_t \\mid s_t)$ for the likelihood of observations given states.\n",
        "\n",
        "However, the agent (e.g., a robot or animal) does not have direct access to this true process. Instead, the agent maintains an internal **generative model**: a probabilistic representation of how it *believes* the world works. This model allows the agent to infer hidden states from observations, predict future outcomes, and plan actions. The generative model is \"specular\" (mirror-like) to the generative process—it has a similar structure—but it represents the agent's subjective beliefs, which may approximate or diverge from the true process.\n",
        "\n",
        "In active inference, we denote the distributions in the generative model with $Q$ to distinguish them from the true $P$ distributions of the process. For example:\n",
        "- $Q(s_{t+1} \\mid s_t, a_t)$ approximates the state transition.\n",
        "- $Q(o_t \\mid s_t)$ approximates the observation likelihood.\n",
        "\n",
        "The key difference:\n",
        "- **Generative Process ($P$)**: The objective, external world dynamics (often unknown to the agent).\n",
        "- **Generative Model ($Q$)**: The agent's internal, subjective model used for inference and decision-making. It is learned or updated over time to better match the process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8006687",
      "metadata": {
        "id": "d8006687"
      },
      "source": [
        "### Factorization Under the Markovian Assumption\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04887514",
      "metadata": {
        "id": "04887514"
      },
      "source": [
        "\n",
        "Under the Markovian assumption (as in the POMDP structure), the generative model factorizes according to the conditional independencies in the causal graph. This means the joint probability breaks down into simpler, local conditional distributions, making computations tractable.\n",
        "\n",
        "Recall the POMDP causal structure: Hidden states evolve based on previous states and actions, and observations depend only on the current hidden state. Actions are chosen based on inferred states (via a policy, which we'll discuss later).\n",
        "\n",
        "The factorization of the generative model is:\n",
        "$$\n",
        "Q(s_{0:T}, a_{0:T-1}, o_{0:T}) = Q(s_0) \\prod_{t=0}^{T-1} Q(s_{t+1} \\mid s_t, a_t) \\cdot Q(a_t \\mid s_t) \\cdot \\prod_{t=0}^{T} Q(o_t \\mid s_t)\n",
        "$$\n",
        "\n",
        "Breaking it down:\n",
        "- $Q(s_0)$: Prior belief over the initial hidden state.\n",
        "- $Q(s_{t+1} \\mid s_t, a_t)$: Believed state transition model (Markovian: depends only on current state and action).\n",
        "- $Q(a_t \\mid s_t)$: The agent's policy, specifying actions given inferred states.\n",
        "- $Q(o_t \\mid s_t)$: Likelihood model (observations depend only on the current hidden state).\n",
        "\n",
        "This factorization mirrors the generative process but uses $Q$ to reflect the agent's approximate beliefs. The Markovian property ensures that each term depends only on immediate predecessors, \"shielding\" the future from the distant past."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a66f7d8",
      "metadata": {
        "id": "2a66f7d8"
      },
      "source": [
        "### Model Inversion: Inferring Hidden States via Bayes' Theorem\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a7d3cb6",
      "metadata": {
        "id": "1a7d3cb6"
      },
      "source": [
        "\n",
        "Once the agent has initialized its **generative model** $Q(s_{0:T}, a_{0:T-1}, o_{0:T})$, its primary task is to **invert** this model to infer the hidden states that best explain the observed data. This inversion is the core of perception in active inference and is performed using **Bayes' theorem**.\n",
        "\n",
        "Let:\n",
        "- $\\underline{o} \\triangleq o_{0:t}$: the sequence of past observations (including current time $t$),\n",
        "- $\\underline{a} \\triangleq a_{0:t-1}$: the sequence of past actions\n",
        "\n",
        "The agent wants to compute the **posterior belief** over the current hidden state $s_t$ (denoted $s_t$ for current time) given all past observations and actions:\n",
        "$$\n",
        "Q(s_t \\mid \\underline{o}, \\underline{a})\n",
        "$$\n",
        "\n",
        "This posterior represents the agent's **inferred belief** about the current state of the world."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73c5a705",
      "metadata": {
        "id": "73c5a705"
      },
      "source": [
        "### Bayesian Model Inversion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f2af25b",
      "metadata": {
        "id": "1f2af25b"
      },
      "source": [
        "\n",
        "Using Bayes' theorem in the generative model:\n",
        "\n",
        "$$\n",
        "\\boxed{\n",
        "Q(s_t \\mid \\underline{o}, \\underline{a})\n",
        "= \\frac{Q(\\underline{o} \\mid \\underline{a}, s_t) \\cdot Q(s_t \\mid \\underline{a})}{Q(\\underline{o} \\mid \\underline{a})}\n",
        "}\n",
        "$$\n",
        "\n",
        "This is **exact Bayesian inference** within the agent's internal model:\n",
        "- $Q(\\underline{o} \\mid \\underline{a}, s_t)$: **Likelihood** — how well the current state would explain the observations (via $Q(o_t \\mid s_t)$).\n",
        "- $Q(s_t \\mid \\underline{a})$: **Prior** — the predicted state under the agent's past actions.\n",
        "- $Q(\\underline{o} \\mid \\underline{a})$: **Model evidence** (marginal likelihood) — normalizes the posterior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "727fe007",
      "metadata": {
        "id": "727fe007"
      },
      "source": [
        "### Exploiting Conditional Independence (Markov Structure)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77ea96a0",
      "metadata": {
        "id": "77ea96a0"
      },
      "source": [
        "\n",
        "\n",
        "Thanks to the **Markovian assumptions** in the generative model, many terms become conditionally independent. In particular:\n",
        "- Each observation $o_t$ depends **only** on its contemporaneous state $s_t$,\n",
        "- Each state transition $s_{t+1}$ depends **only** on $s_t$ and $a_t$.\n",
        "\n",
        "This allows us to simplify the likelihood:\n",
        "$$\n",
        "Q(\\underline{o} \\mid \\underline{a}, s_t) = Q(\\underline{o} \\mid s_t) = \\prod_{k=0}^{\\tau-1} Q(o_k \\mid s_k)\n",
        "$$\n",
        "\n",
        "So the full posterior becomes:\n",
        "\n",
        "$$\n",
        "\\boxed{\n",
        "Q(s_t \\mid \\underline{o}, \\underline{a})\n",
        "= \\frac{\n",
        "\\overbrace{Q(\\underline{o} \\mid s_t)}^{\\text{likelihood over all obs}}\n",
        "\\cdot\n",
        "\\overbrace{Q(s_t \\mid \\underline{a})}^{\\text{prior state distribution}}\n",
        "}{\n",
        "\\underbrace{Q(\\underline{o} \\mid \\underline{a})}_{\\text{marginal likelihood or evidence}}\n",
        "}\n",
        "} \\tag{1}\n",
        "$$\n",
        "\n",
        "The **marginal likelihood** of the observations under the agent’s generative model, in principle is the result of a marginalisation (hence the name):\n",
        "\n",
        "$$\n",
        "Q(\\underline{o} \\mid \\underline{a}) = \\int Q(\\underline{o}, s_t \\mid \\underline{a}) \\, ds_t\n",
        "= \\int \\underbrace{Q(\\underline{o} \\mid s_t)}_{\\text{likelihood}} \\cdot \\underbrace{Q(s_t \\mid \\underline{a})}_{\\text{prior over trajectories}} \\, ds_t\n",
        "$$\n",
        "\n",
        "This is a **high-dimensional integral** over all possible states sequences $s_t$. For even moderately discrete or continuous state spaces, this integral is **intractable** — it cannot be computed exactly.\n",
        "\n",
        "Thus, **exact Bayesian inference** is generally infeasible in real-time agents."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82fb9fc0",
      "metadata": {
        "id": "82fb9fc0"
      },
      "source": [
        "### A useful approximation:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e462c413",
      "metadata": {
        "id": "e462c413"
      },
      "source": [
        "\n",
        "To make inference tractable, we introduce a **_variational_ distribution** $q_{\\underline{o}}(s_t\\mid \\underline{a})$ — a simpler, factorized family of distributions whose PDF will be a function of $\\underline{o}$. (We would say that $\\underline{o}$ are sufficient statistics for $q$). This parameterized distribution **approximates** the true posterior $Q(s_t \\mid \\underline{o}, \\underline{a})$. We restrict $q_{\\underline{o}}(s_t\\mid \\underline{a})$ to a family of distributions that are easy to evaluate and update (e.g., mean-field, Gaussian, recurrent neural network approximations).\n",
        "\n",
        "The goal: **minimize the difference** between $q_{\\underline{o}}(s_t\\mid \\underline{a})$ and the true posterior:\n",
        "\n",
        "$$\n",
        "    D_{KL}\\left[ q_{\\underline{o}}(s_t\\mid \\underline{a}) || Q(s_t \\mid \\underline{o}, \\underline{a})  \\right]   \\rightarrow 0 \\tag{2}\n",
        "$$\n",
        "\n",
        "But we can't directly minimise this divergence because the true posterior, $ Q(s_t \\mid \\underline{o}, \\underline{a})$ is unknown because it itself depends on the intractable evidence $Q(\\underline{o} \\mid \\underline{a})$!\n",
        "\n",
        "(Notice that, if $ Q(s_t \\mid \\underline{o}, \\underline{a})$ were computable, we would be talking about variational inference at all!)\n",
        "\n",
        "So we will minimise $(2)$ indirectly by minimising a quantity that we call Variational Free Energy. To see how, let's take a closer look to the KL divergence in $(2)$:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bad8dc72",
      "metadata": {
        "id": "bad8dc72"
      },
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "    D_{KL}\\left[ q_{\\underline{o}}(s_t\\mid \\underline{a}) || Q(s_t \\mid \\underline{o}, \\underline{a})  \\right]   &=\n",
        "    \\mathbb{E}_{q_{\\underline{o}}(s_t\\mid \\underline{a})}\\left[ \\ln q_{\\underline{o}}(s_t\\mid \\underline{a}) - \\ln Q(s_t \\mid \\underline{o}, \\underline{a}) \\right]\\\\\\\\\n",
        "    &\\text{using the definition of conditional probability: }\\\\\n",
        "    &\\left( Q(s_t \\mid \\underline{o}, \\underline{a})=\n",
        "                \\frac{\n",
        "            Q(\\underline{o}, s_t \\mid \\underline{a})\n",
        "            }{\n",
        "            Q(\\underline{o} \\mid \\underline{a})\n",
        "            }\n",
        "    \\right)\\\\\n",
        "    &\\left( \\ln Q(s_t \\mid \\underline{o}, \\underline{a})=\n",
        "            \\ln Q(\\underline{o}, s_t \\mid \\underline{a}) - \\ln Q(\\underline{o} \\mid \\underline{a})\\right) \\\\\\\\\n",
        "     &\\text{substituting this into the KL divergence:}\\\\\n",
        "    &=\\mathbb{E}_{q_{\\underline{o}}(s_t\\mid \\underline{a})}\\left[ \\ln q_{\\underline{o}}(s_t\\mid \\underline{a}) - \\ln Q(\\underline{o}, s_t \\mid \\underline{a}) + \\ln Q(\\underline{o} \\mid \\underline{a}) \\right]\\\\\\\\\n",
        "    &\\text{The third term can get out of the expectation:}\\\\\n",
        "    &=\\mathbb{E}_{q_{\\underline{o}}(s_t\\mid \\underline{a})}\\left[ \\ln q_{\\underline{o}}(s_t\\mid \\underline{a}) -\\ln Q(\\underline{o}, s_t \\mid \\underline{a})\\right] + \\ln Q(\\underline{o} \\mid \\underline{a}) \\\\\\\\\n",
        "    &\\text{By definition of KL divergence:}\\\\\n",
        "    &=D_{KL}\\left[ q_{\\underline{o}}(s_t\\mid \\underline{a}) || Q(\\underline{o}, s_t \\mid \\underline{a})\\right]+ \\ln Q(\\underline{o} \\mid \\underline{a})\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fda09d2b",
      "metadata": {
        "id": "fda09d2b"
      },
      "source": [
        "Let's look again to the identity we have just found, let's add some names to the terms in this expression:\n",
        "\n",
        "$$\n",
        "    \\underbrace{D_{KL}\\left[ q_{\\underline{o}}(s_t\\mid \\underline{a}) || Q(s_t \\mid \\underline{o}, \\underline{a})  \\right]}_{\\text{Approximation error}}  =\n",
        "    \\underbrace{D_{KL}\\left[ q_{\\underline{o}}(s_t\\mid \\underline{a}) || Q(\\underline{o}, s_t \\mid \\underline{a})\\right]}_{\\textit{Variational Free Energy}}+ \\underbrace{\\ln Q(\\underline{o} \\mid \\underline{a})}_{\\text{log evidence}}\n",
        "$$\n",
        "\n",
        "This tell's us that, even if we are not able to minimise the approximation error directly, **we can minimise the approximation error vicariously by minimising Variational Free Energy (VFE)**, whatever the VFE actually is.\n",
        "\n",
        "Let's also put in evidente the log-evidence, which we know is a good thing to maximise:\n",
        "\n",
        "$$\n",
        "\n",
        "    \\underbrace{\\ln Q(\\underline{o} \\mid \\underline{a})}_{\\text{log evidence}} =  \\underbrace{D_{KL}\\left[ q_{\\underline{o}}(s_t\\mid \\underline{a}) || Q(s_t \\mid \\underline{o}, \\underline{a})  \\right]}_{\\text{Approximation error}}  - \\underbrace{D_{KL}\\left[ q_{\\underline{o}}(s_t\\mid \\underline{a}) || Q(\\underline{o}, s_t \\mid \\underline{a})\\right]}_{\\textit{Variational Free Energy}}\n",
        "$$\n",
        "\n",
        "Looking things from this point of view, we are even more convinced that minimising the VFE is actually a good idea: it will maximise the evidence of our generative model!\n",
        "\n",
        "Now, the only question is, what a heck is the VFE? Let's put a second in evidence this term:\n",
        "\n",
        "$$\n",
        "    \\underbrace{D_{KL}\\left[ q_{\\underline{o}}(s_t\\mid \\underline{a}) || Q(\\underline{o}, s_t \\mid \\underline{a})\\right]}_{\\textit{Variational Free Energy}}\n",
        "    =  \\underbrace{D_{KL}\\left[ q_{\\underline{o}}(s_t\\mid \\underline{a}) || Q(s_t \\mid \\underline{o}, \\underline{a})  \\right]}_{\\text{Approximation error}} - \\underbrace{\\ln Q(\\underline{o} \\mid \\underline{a})}_{\\text{\\textit{log evidence}}}  \n",
        "$$\n",
        "\n",
        "We now see hoe the VFE is an upper bound on the negative of the log evidence (which is also called surprisal). If we need to maximise evidence to construct a good generative model, we should minimise surprisal. Again, this expression convinces us that, by minimising the VFE _functional_ (a functional is a function of a function), we are vicariously minimising surprise, because the VFE is an upper bound on it. Conversely, if we minimise VFE we maximise Evidence because the VFE is a lower bound of it. (in fact, ML fellas call the \"Evidence Lower Bound\" or ELBo to the negative of VFE)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2464c25",
      "metadata": {
        "id": "f2464c25"
      },
      "source": [
        "## The Variational Free Energy:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "539ec987",
      "metadata": {
        "id": "539ec987"
      },
      "source": [
        "The Variational Free Energy is a KL divergence:\n",
        "\n",
        "$$\n",
        "VFE = D_{KL}\\left[ q_{\\underline{o}}(s_t\\mid \\underline{a}) || Q(\\underline{o}, s_t \\mid \\underline{a}) \\right] \\tag{3}\n",
        "$$\n",
        "\n",
        "There are many ways to understand such a divergence, depending on how we factorise it:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "VFE &= D_{KL}\\left[ q_{\\underline{o}}(s_t\\mid \\underline{a}) || Q(\\underline{o}, s_t \\mid \\underline{a}) \\right] \\\\\n",
        "&\n",
        "= \\mathbb{E}_{q_{\\underline{o}}(s_t\\mid \\underline{a})}\\left[ \\ln q_{\\underline{o}}(s_t\\mid \\underline{a}) - \\ln Q(\\underline{o}, s_t \\mid \\underline{a}) \\right] \\\\\n",
        "& = - \\underbrace{\\mathbb{H}[q_{\\underline{o}}(s_t\\mid \\underline{a})]}_{\\text{Neutrality}} - \\underbrace{\\mathbb{E}_{q_{\\underline{o}}(s_t\\mid \\underline{a})}[Q(\\underline{o}, s_t \\mid \\underline{a})}_{\\text{Consistency}}]\n",
        "\\end{aligned}\n",
        "\\tag{3.1}\n",
        "$$\n",
        "\n",
        "The first way of understanding the VFE $(3.1)$ is looking it as the summation of two terms:\n",
        "- the negative entropy of the variational distribution: The entropy is a measure of neutrality, a measure of spread. The more entropy, the less specific our distribution is, and so it may accomodate a bigger set of distributions. We should maximise the entropy a priori. This is a well-noted principle in many fields of physics, neuroscience and information theory. By maximising this entropy or neutrality of first guesses, we are minimising the VFE, which we know is a good thing to do.\n",
        "\n",
        "- the negative crossentropy of the joint with respect to the variational distribution. This thing should be maximised, because joint used here is supposed to be normative, i.e. it encodes the states, behaviour and observations the agent should have. This cross-entropy thus encodes a more pragmatic goal. Maximises this cross-entropy also minimises the VFE, so we are good to do so."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94f08b60",
      "metadata": {
        "id": "94f08b60"
      },
      "source": [
        "Let's look at a second way of expressing the VFE:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "VFE &= D_{KL}\\left[ q_{\\underline{o}}(s_t\\mid \\underline{a}) || Q(\\underline{o}, s_t \\mid \\underline{a}) \\right] \\\\\n",
        "&\n",
        "= \\mathbb{E}_{q_{\\underline{o}}(s_t\\mid \\underline{a})}\\left[ \\ln q_{\\underline{o}}(s_t\\mid \\underline{a}) - \\ln Q(\\underline{o}, s_t \\mid \\underline{a}) \\right] \\\\\n",
        "& = \\mathbb{E}_{q_{\\underline{o}}(s_t\\mid \\underline{a})}\\left[ \\ln q_{\\underline{o}}(s_t\\mid \\underline{a}) - \\ln Q(\\underline{o} \\mid s_t) - \\ln Q(s_t \\mid \\underline{a})\\right] \\\\\n",
        "& = \\mathbb{E}_{q_{\\underline{o}}(s_t\\mid \\underline{a})}\\left[ \\ln q_{\\underline{o}}(s_t\\mid \\underline{a})  - \\ln Q(s_t \\mid \\underline{a}) \\right]- \\mathbb{E}_{q_{\\underline{o}}(s_t\\mid \\underline{a})}\\left[\\ln Q(\\underline{o} \\mid s_t)\\right] \\\\\n",
        "& =  \\underbrace{D_{KL} \\left[ q_{\\underline{o}}(s_t\\mid \\underline{a}) || Q(s_t \\mid \\underline{a}) \\right]}_{\\text{Complexity}} - \\underbrace{\\mathbb{E}_{q_{\\underline{o}}(s_t\\mid \\underline{a})}\\left[ \\ln Q(\\underline{o} \\mid s_t) \\right]}_{\\text{Accuracy}} \\\\\n",
        "\\end{aligned}\n",
        "\\tag{3.2}\n",
        "$$\n",
        "\n",
        "- here the complexity term is measuring the divergence between our approximate posterior and the form of the prior. Again, here the prior is assumed to be our goal, our guide. So we are measuring here how much \"work\" we should do to build a posterior  (i.e. a prior informed by evidence) whic fits with the real distribution of states given the same actions. Minimising such divergence is a good thing to do if we consider that the actions are already taken so, irrespective of the evidence, the final state we are in should be the same if we take the same actions...\n",
        "- The accuracy is more of an expectation of evidence. Again, the distribution whose expectation we are computing, $Q(\\underline{o} \\mid s_t)$ is a normative one, like a ground thruth that tell's us what is really the distribution of ovservations we should expect when beign in state $s_t$, it is a good thing if the probability of $\\underline{o}$ is maximised when the expcation is computed over $q_{\\underline{o}}(s_t\\mid \\underline{a})$, i.e., when we are using $\\underline{o}$ to build our approximate posterior.\n",
        "\n",
        "> We will use eiter $(3.1)$ or $(3.2)$ in our experiments as a ground truth signal that gives us a bayes optimal way of using our generative model at each moment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92b92382",
      "metadata": {
        "id": "92b92382"
      },
      "source": [
        "## The Expected Free Energy (EFE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8269c4da",
      "metadata": {
        "id": "8269c4da"
      },
      "source": [
        "So far so good, we've seen how to build a structure for implementing an agent (the MDP), how to make that structure more compliant with uncertainty (the POMDP), and how to update the generative model of an agent (minimisng the VFE).\n",
        "\n",
        "Now the thing is, we've so far talked about a retroscpectively updating such a model to be consistent with a series of PAST observations, actions and states.\n",
        "\n",
        "It turns out that an AGENT also ACTS, i.e., it not only passively observes \"evidence\" but it could also actively \"drive\" the course of its future observations.\n",
        "\n",
        "Taking into account this prospective side of agency makes things interesting when adopting the bayesian framework we are doing.\n",
        "\n",
        "In particular, we could impose a distribution of observations that periodically our agent oughts to see, being aware of the fact that he cannot directly steer its observations but can only vicariously drive these through actions.\n",
        "\n",
        "By using this laanguage, we can say that an intelligent or compliant agent is the one who acts to succesfully achieve some observations. This formulation is akin to cybernetics, or, in other words, the fisiology of artificial agents: To exist, an agent has to maintain a distribution of observations in its steady state, admist the fluctuations derived form the environment that would drive it to \"death\" or \"malfunction\" unless it actively mitigates this phenomena.\n",
        "\n",
        "In other words, when we speak about the next or future actions, we could use our VFE bayesian machinery to choose the best actions to make as beign the ones which will minimise the VFE in the future. The VFE in the future is called the Expected Free Energy and is the last functional we study in this tutorial as a criterion for policy shaping, i.e., for choosing actions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cefdc12a",
      "metadata": {
        "id": "cefdc12a"
      },
      "source": [
        "### Looking forward"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d09efa3",
      "metadata": {
        "id": "1d09efa3"
      },
      "source": [
        "The expected free energy can be seen as the expectation of variational free energy over a sequence of future states and observations before we actually see them. The unique thing to keep in mind is that these future observations are provoked by a sequence of actions, and that the future observations themselves provoke a future sequence of states. To keep such a causality chain as correct as possible we chiarify were we are now in terms of time:\n",
        "\n",
        "- suppose we are now at instant $t$.\n",
        "- we want to plan the future $h$ actions. (we say that $h$ is our _planning horizon_)\n",
        "- we can see $o_t$, and we can _infer_ $s_t$ using model inversion, as explained before.\n",
        "- we want to choose the next action, which is actually $a_t$ (not $a_{t+1}$!, cuz initial state was $s_0$, initial obsrvation was $o_0$, and the initial action, taken while being at $s_0$ and after bserving $o_0$ was $a_0$. See? actions are one-step backwards... Actually $a_0$ provokes $s_1$ and $s_1$ provokes $o_1$...)\n",
        "- if we where to choose not only our next action, $a_t$ but a next sequence of $h$ actions, then we want to choose $a_t, a_{t+1}, a_{t+2},... a_{t+h}$\n",
        "- so we denote $\\bar{a} = \\{a_t,a_{t+1},...a_{t+h}\\}$ and those fellas will generate $s_{t+1}, s_{t+2}, ..., s_{t+h+1}$\n",
        "- so we use $\\bar{s} = s_{t+1:t+h+1}$ for the sequence of future latent states in the planning horizon.\n",
        "- those latent states will generate  $o_{t+1}, o_{t+2}, ..., o_{t+h+1}$, so we use $\\bar{o} = o_{t+1:t+h+1}$ denote the sequence of the future observations.\n",
        "\n",
        "Summary:\n",
        "\n",
        "$$\n",
        "\\boxed{\n",
        "\\begin{aligned}\n",
        "\\bar{a} &\\triangleq  a_{t:t+h}\\\\\n",
        "\\bar{s} &\\triangleq  s_{t+1:t+h+1}\\\\\n",
        "\\bar{o} &\\triangleq  o_{t+1:t+h+1}\\\\\n",
        "\\end{aligned}\n",
        "}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5400ccc8",
      "metadata": {
        "id": "5400ccc8"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Recall that the VFE is an expectation over the distribuion $q_{\\underline{o}}(\\mathbf{s_t} | \\underline{a})$, which depends on a sequence of past actions, $\\underline{a}$, and a sequence of past observations, $\\underline{o}$.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "VFE &= D_{KL}\\left[ q_{\\underline{o}}(s_t\\mid \\underline{a}) || Q(\\underline{o}, s_t \\mid \\underline{a}) \\right] \\\\\n",
        "&=\\mathbb{E}_{q_{\\underline{o}}(s_t\\mid \\underline{a})}\\left[ \\ln q_{\\underline{o}}(s_t\\mid \\underline{a}) - \\ln Q(\\underline{o}, s_t \\mid \\underline{a}) \\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "The EFE is a kind of expectation of the VFE under a new distribution, namely, $Q( \\bar{o}, \\bar{s} | \\bar{a})$, which is the joint distribution of a future sequence of observations, $\\bar{o}$, and a future sequence of states $\\bar{s}$, given a future sequence of actions $\\bar{a}$:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "VFE &= \\mathbb{E}_{q_{\\underline{o}}(s_t\\mid \\underline{a})}\\left[ \\ln q_{\\underline{o}}(s_t\\mid \\underline{a}) - \\ln Q(\\underline{o}, s_t \\mid \\underline{a}) \\right]\\\\\\\\\n",
        "&\\text{Taking an expectation of VFE over the future:}\\\\\n",
        "EFE &\\leftarrow \\mathbb{E}_{Q( \\bar{o}, \\bar{s} | \\bar{a})}[VFE] \\\\\n",
        "EFE &\\leftarrow \\mathbb{E}_{Q( \\bar{o}, \\bar{s} | \\bar{a})}\\left[\\mathbb{E}_{q_{\\underline{o}}(s_t\\mid \\underline{a})}\\left[ \\ln q_{\\underline{o}}(s_t\\mid \\underline{a}) - \\ln Q(\\underline{o}, s_t \\mid \\underline{a}) \\right]\\right]\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a773c6e",
      "metadata": {
        "id": "0a773c6e"
      },
      "source": [
        "We are basically computing an expectation of the variational free energy that would result out of a sequence of future observations and states (generated by a sequence of future actions) under our current generative model.\n",
        "\n",
        "One thing we can notice is that, in the future, $\\underline{o}$ will be what in the present we refer to as $\\bar{o}$. Which, in other words, can be rephrased as: \"the future observations, in the future, will be the present observations\". Not only, but also:\n",
        "\n",
        "> If we are in instant $t$, we can say that what we call \"the future $h$ observations now\", i.e. $\\bar{o}\\triangleq  o_{t+1:t+h+1}$, will be equivalent to the \"past $h$ observations\", i.e. $\\underline{o}$ at instant $t+h+1$. The same will hold for states, i.e. $\\bar{s}$ in the present is equivalent to $\\underline{s}$ if talking from $h+1$ steps in the future.\n",
        "\n",
        "Having this in mind, let's look for a second to the VFE at instant $t+h+1$:\n",
        "$$\n",
        "VFE_{\\{t+h+1\\}}= D_{KL}\\left[ q_{\\bar{o}}(s_{t+h+1}\\mid \\bar{a}) || Q(\\bar{o}, s_{t+h+1} \\mid \\bar{a}) \\right]\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50ce4efe",
      "metadata": {
        "id": "50ce4efe"
      },
      "source": [
        "The last observation, and the fact that $\\bar{s}$ actually contains $s_{t+h+1}$ itself, _seems to license us_ to make an approximation:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\mathbb{E}_{Q( \\bar{o}, \\bar{s} | \\bar{a})}\\left[\\mathbb{E}_{q_{\\underline{o}}(s_t\\mid \\underline{a})}\\left[ \\ln q_{\\underline{o}}(s_t\\mid \\underline{a}) - \\ln Q(\\underline{o}, s_t \\mid \\underline{a}) \\right]\\right]\\\\\\\\\n",
        "&= \\sum_{\\tau \\in \\{t,..., t+h\\}} \\sum_{\\{o_{\\tau+1}, \\,s_{\\tau+1},\\, a_{\\tau}\\}}  Q(o_{\\tau+1}, s_{\\tau+1} | a_{\\tau})  \\left[ \\ln q_{o_{\\tau+1}}(s_{\\tau+1}\\mid a_{\\tau}) - \\ln Q(o_{\\tau+1}, s_{\\tau+1} \\mid a_{\\tau}) \\right] \\\\\\\\\n",
        "&= \\mathbb{E}_{Q( \\bar{o}, \\bar{s} | \\bar{a})}\\left[ \\ln q_{\\bar{o}}(\\bar{s}\\mid \\bar{a}) - \\ln Q(\\bar{o}, \\bar{s} \\mid \\bar{a}) \\right]\\\\\\\\\n",
        "&\\text{Which holds by virtue of our variational approximation:}\\\\\n",
        "&Q( o_{\\tau+1}, s_{\\tau+1} | a_{\\tau}) = Q(s_{\\tau+1}  | o_{\\tau+1}, a_{\\tau})\\,Q( o_{\\tau+1} | a_{\\tau}) \\approx q_{o_{\\tau+1}}(s_{\\tau+1}  | a_{\\tau})\\,Q( o_{\\tau+1} | a_{\\tau})\\\\\n",
        "&\\text{in other words:} \\quad Q( \\bar{o}, \\bar{s} | \\bar{a}) = Q( \\bar{s} | \\bar{o}, \\bar{a})\\,Q( \\bar{o} | \\bar{a}) \\approx q_{\\bar{o}}(\\bar{s} | \\bar{a})\\,Q( \\bar{o}| \\bar{a})\\\\\\\\\n",
        "&\\text{Factorising the second term in the expectation using the product rule:}\\\\\n",
        "&= \\mathbb{E}_{Q( \\bar{o}, \\bar{s} | \\bar{a})}\\left[ \\ln q_{\\bar{o}}(\\bar{s}\\mid \\bar{a}) - \\ln \\left(Q(\\bar{s} | \\bar{o} , \\bar{a})  Q(\\bar{o} \\mid \\bar{a})  \\right)  \\right]\\\\\\\\\n",
        "&\\text{Using the properties of logarithms we have:}\\\\\n",
        "&= \\mathbb{E}_{Q( \\bar{o}, \\bar{s} | \\bar{a})}\\left[ \\ln q_{\\bar{o}}(\\bar{s}\\mid \\bar{a}) -\\left( \\ln Q(\\bar{s} | \\bar{o} , \\bar{a}) + \\ln Q(\\bar{o} \\mid \\bar{a})  \\right)  \\right]\\\\\n",
        "&= \\mathbb{E}_{Q( \\bar{o}, \\bar{s} | \\bar{a})}\\left[ \\ln q_{\\bar{o}}(\\bar{s}\\mid \\bar{a}) - \\ln Q(\\bar{s} | \\bar{o} , \\bar{a}) - \\ln Q(\\bar{o} \\mid \\bar{a}) \\right]\\\\\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3447a2b0",
      "metadata": {
        "id": "3447a2b0"
      },
      "source": [
        "At this point things become extremely interesting, as both our fantasy and our empirical experience will give both a useful interpretation and a useful directionaluìity to this expression for the sake of letting it be a functional block of intelligent agency."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "790db874",
      "metadata": {
        "id": "790db874"
      },
      "source": [
        "### Self-evidencing:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d598310b",
      "metadata": {
        "id": "d598310b"
      },
      "source": [
        "Fisiologists say living organisms possess homeostatic equilibrium. In other words, living organisms remain alive when they counteract termodynamical fluctuations with their environment by maintaining their internal state in some permissibile or vital range or field. In terms of \"state spaces\" (i.e. in terms of measurable approximations of what would be an \"internal state\") we would say that each agent should have a statistical distribution of observations $o$ which characterises it. This means that, if I would sample the agents observations at any time, such a sampling should be from the distribution of the agent's vital distribution. We would call this statistical distribution $P(o|\\mathbf{C})$, as if $\\mathbf{C}$ where the conditioning event that tells us the agent is working or existing.\n",
        "\n",
        "Synthesising, we are saying that our paradigm of creation of intelligent agents would be that of imposing for each of them a working or existing specification $\\mathbf{C}$ that shapes the distribution of observations it has to receive at each moment. For example, if we are creating and artificial fish, $P(o|\\mathbf{C})$ would rather correspond to observations from water that from the sky, or, if we are creating an artificial human being,  $P(o|\\mathbf{C})$ would rather correspond to (proprioceptive) observations of an internal temperature below 40 Celsius degrees that above...\n",
        "\n",
        "This process of imposing a distribution of vital observations is called \"self-evidencing\". Actually, fisiology wouls say that, under the EFE formalism, biological living organisms appear _as if_ they maximise the evidence of observations described by $P(o|\\mathbf{C})$.\n",
        "\n",
        "These self-evidencing dynamics can actually be encoded in the bayesian formalism we have (approximately) derive so far."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6eb26cef",
      "metadata": {
        "id": "6eb26cef"
      },
      "source": [
        "Recall the EFE is getting a kind of shape that looks like:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "EFE \\leftarrow \\mathbb{E}_{Q( \\bar{o}, \\bar{s} | \\bar{a})}\\left[ \\ln q_{\\bar{o}}(\\bar{s}\\mid \\bar{a}) - \\ln Q(\\bar{s} | \\bar{o} , \\bar{a}) - \\ln Q(\\bar{o} \\mid \\bar{a}) \\right]\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Now remember that what we are trying to get out from all this is a piece of criterion for an agent to choose actions that shape the state of its environment, cuz the state of its environment is what generates the observations. Why? Because we assume that (intelligent) agents want to get good observations.\n",
        "\n",
        "What are good observations? It depends on the agent's goal, an agent that works for finance might want to maximise incomes, an agent that is a heating system might want to keep the temperature of a room in a certain range of values, etc...\n",
        "\n",
        "What the active inference paradigm says is: do not encode good observations as (potentially extrinsic, hand-made) rewards, as the majority of reinforcement learning fellas do, but rather encode them as vital observations, encode them in $\\mathbf{C}$! So encode \"intelligence\" as some kind of (digital) \"homeostasis\"!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e1b2734",
      "metadata": {
        "id": "4e1b2734"
      },
      "source": [
        "> In this vein, we can say \"ok, so the best actions, $a^*$ are those that shape the state $s$ in such a way that is produces observations sampled from $P(o|\\mathbf{C})$.\n",
        "\n",
        "Yep. This is a good criterion for choosing actions. Now what does it has to do with our EFE formalism? The answer follows:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\text{This is our previous EFE embryonic expression:}\\\\\n",
        "EFE &\\leftarrow \\mathbb{E}_{Q( \\bar{o}, \\bar{s} | \\bar{a})}\\left[ \\ln q_{\\bar{o}}(\\bar{s}\\mid \\bar{a}) - \\ln Q(\\bar{s} | \\bar{o} , \\bar{a}) - \\ln Q(\\bar{o} \\mid \\bar{a}) \\right]\\\\\\\\\n",
        "&\\text{This is our following EFE expression, where we impose \"self-evidencing\":}\\\\\n",
        "EFE &\\leftarrow \\mathbb{E}_{Q( \\bar{o}, \\bar{s} | \\bar{a})}\\left[ \\ln q_{\\bar{o}}(\\bar{s}\\mid \\bar{a}) - \\ln Q(\\bar{s} | \\bar{o} , \\bar{a}) - \\ln Q(\\bar{o} \\mid \\mathbf{C}) \\right]\\\\\\\\\n",
        "&\\text{Let's separate the terms of the expectation for interpreting better the result:}\\\\\n",
        "EFE &\\leftarrow \\mathbb{E}_{Q( \\bar{o}, \\bar{s} | \\bar{a})}\\left[ \\ln q_{\\bar{o}}(\\bar{s}\\mid \\bar{a}) - \\ln Q(\\bar{s} | \\bar{o} , \\bar{a}) \\right] - \\mathbb{E}_{Q( \\bar{o}, \\bar{s} | \\bar{a})}\\left[ \\ln Q(\\bar{o} \\mid \\mathbf{C}) \\right]\\\\\\\\\n",
        "&\\text{Recall the corollary of the variational approx:}\\\\\n",
        "&Q( \\bar{o}, \\bar{s} | \\bar{a}) = Q( \\bar{s} | \\bar{o}, \\bar{a})\\,Q( \\bar{o} | \\bar{a}) \\approx q_{\\bar{o}}(\\bar{s} | \\bar{a})\\,Q( \\bar{o}| \\bar{a})\\\\\\\\\n",
        "&\\text{By which:}\\\\\n",
        "EFE &\\leftarrow \\mathbb{E}_{Q( \\bar{o}, \\bar{s} | \\bar{a})}\\left[ \\ln q_{\\bar{o}}(\\bar{s}\\mid \\bar{a}) - \\ln Q(\\bar{s} | \\bar{o} , \\bar{a}) \\right] - \\mathbb{E}_{q_{\\bar{o}}(\\bar{s} | \\bar{a})\\,Q( \\bar{o}| \\bar{a})}\\left[ \\ln Q(\\bar{o} \\mid \\mathbf{C}) \\right]\\\\\\\\\n",
        "&\\text{Keeping only the distributions upon which the expectation of the last expression actually depends we have:}\\\\\n",
        "EFE &\\leftarrow \\mathbb{E}_{Q( \\bar{o}, \\bar{s} | \\bar{a})}\\left[ \\ln q_{\\bar{o}}(\\bar{s}\\mid \\bar{a}) - \\ln Q(\\bar{s} | \\bar{o} , \\bar{a}) \\right] - \\underbrace{\\mathbb{E}_{Q( \\bar{o}| \\bar{a})}\\left[ \\ln Q(\\bar{o} \\mid \\mathbf{C}) \\right]}_{\\text{The \"Active\" part in Active Inference!}}\\\\\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bc04ef7",
      "metadata": {
        "id": "0bc04ef7"
      },
      "source": [
        "> What have we done? We have just noted that, if we want to minimise the VFE to update a good generative model, then choosing actions that minimise the EFE will have the same effect. **BUT WE DO NOT STOP HERE:** If the EFE minimisation dynamics could also maximise the accordance/accuracy/cross-entropy between our generative model and the vital observations descrived in $P(\\bar{o}|\\mathbf{C})$ we will not only have a good generative model, but also a god **ACTIVE MODEL! A REALLY \"INTELLIGENT\" or, at least, a \"GOAL-DIRECTED\" AGENT!**\n",
        "\n",
        "The third term in the previous expression is thus said to quantify the _expected pragmatic gain_ of our future sequence of actions given our generative model. In that the \"pragmatic\" adjective is related to the alignment with a concrete goal:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "EFE &\\leftarrow \\mathbb{E}_{Q( \\bar{o}, \\bar{s} | \\bar{a})}\\left[ \\ln q_{\\bar{o}}(\\bar{s}\\mid \\bar{a}) - \\ln Q(\\bar{s} | \\bar{o} , \\bar{a}) \\right] - \\underbrace{\\mathbb{E}_{Q( \\bar{o}| \\bar{a})}\\left[ \\ln Q(\\bar{o} \\mid \\mathbf{C}) \\right]}_{\\text{Pragmatic Gain}}\\\\\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "In the following part, we will do a final modification to the evolutioning expression of the EFE to actually define it the way Friston does."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2a4a720",
      "metadata": {
        "id": "b2a4a720"
      },
      "source": [
        "### Artificial Curiosity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63709631",
      "metadata": {
        "id": "63709631"
      },
      "source": [
        "We have done two things so far:\n",
        "    - Use the \"minimisation of bayesian surprise\" criterion to maximise the accuracy of our generative model.\n",
        "    - Use the \"maximisation of homeostatic steady state\" criterion to maximise the goal-directed behaviour of our agent."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38de992b",
      "metadata": {
        "id": "38de992b"
      },
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "EFE &\\leftarrow \\mathbb{E}_{Q( \\bar{o}, \\bar{s} | \\bar{a})}\\left[ \\ln q_{\\bar{o}}(\\bar{s}\\mid \\bar{a}) - \\ln Q(\\bar{s} | \\bar{o} , \\bar{a}) \\right] - \\mathbb{E}_{Q( \\bar{o}| \\bar{a})}\\left[ \\ln Q(\\bar{o} \\mid \\mathbf{C}) \\right]\\\\\\\\\n",
        "&\\text{Recalling that:}\\\\\n",
        "(*) &\\quad Q( \\bar{o}, \\bar{s} | \\bar{a}) = Q( \\bar{s} | \\bar{o}, \\bar{a})\\,Q( \\bar{o} | \\bar{a})\\\\\\\\\n",
        "&\\text{We can decompose the first expectation into a nested one:}\\\\\n",
        "EFE &\\leftarrow \\mathbb{E}_{Q( \\bar{o} | \\bar{a})}\\left[\\mathbb{E}_{ Q( \\bar{s} | \\bar{o}, \\bar{a})}\\left[ \\ln q_{\\bar{o}}(\\bar{s}\\mid \\bar{a}) - \\ln Q(\\bar{s} | \\bar{o} , \\bar{a}) \\right]\\right] - \\mathbb{E}_{Q( \\bar{o}| \\bar{a})}\\left[ \\ln Q(\\bar{o} \\mid \\mathbf{C}) \\right]\\\\\\\\\n",
        "&\\text{Rearranging the terms in the first expectation for convenience:}\\\\\n",
        "EFE &\\leftarrow - \\mathbb{E}_{Q( \\bar{o} | \\bar{a})}\\left[\\mathbb{E}_{ Q( \\bar{s} | \\bar{o}, \\bar{a})}\\left[  \\ln Q(\\bar{s} | \\bar{o} , \\bar{a}) - \\ln q_{\\bar{o}}(\\bar{s}\\mid \\bar{a}) \\right]\\right] - \\mathbb{E}_{Q( \\bar{o}| \\bar{a})}\\left[ \\ln Q(\\bar{o} \\mid \\mathbf{C}) \\right]\\\\\\\\\n",
        "&\\text{The inner expectation is a KL divergence:}\\\\\n",
        "EFE &\\leftarrow - \\mathbb{E}_{Q( \\bar{o} | \\bar{a})}\\left[D_{KL}\\left[Q(\\bar{s} | \\bar{o} , \\bar{a}) || q_{\\bar{o}}(\\bar{s}\\mid \\bar{a}) \\right]\\right] - \\mathbb{E}_{Q( \\bar{o}| \\bar{a})}\\left[ \\ln Q(\\bar{o} \\mid \\mathbf{C}) \\right]\\\\\\\\\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15d8df77",
      "metadata": {
        "id": "15d8df77"
      },
      "source": [
        "Now I bet y'all are asking why I didn't use the variational approximation in (*). The answer is that the \"real posterior\", $Q(\\bar{s} | \\bar{o} , \\bar{a})$, in the future, is actually computable or, at least, can be approximated differently from the (current) approximate $q_{\\bar{o}}(\\bar{s}\\mid \\bar{a})$ guy.\n",
        "\n",
        "In principle, what we are putting in evidence by the dynamics in the KL divergence between the real future and the approximate future posterior is a quantification of how different is my approximation of the future posterior with respect to the actual future posterior. (We know this, is obvious) But the interesting part is that this whole divergence is the argument of an expectation over the future sequence of observations given actions: this means that we are quantifying how much, in expectation, the future of pursuing a sequence of actions $\\bar{a}$ is gonna drive change to my current approximation of the posterior. How much my approximate posterior is gonna change after observing that bunch of observations that it will observe after taking that bunch of actions, like, how much I think the exploration I am planning to do will change my mind or my model about how the states generate observations given actions!\n",
        "\n",
        "But we have not finished praising this expression. The fact is that, if we impose we wanna minimise the EFE fella, then we are saying that our agent not only will select goal-directed actions (as for maximising the pragmatic gain), but we also will like to MAXIMISE the expected \"information gain\" of \"change of mind\" expressed by the first term! (Maximising that guy will lead EFE to drop cuz it has a \"minus\" right before...)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b09caf29",
      "metadata": {
        "id": "b09caf29"
      },
      "source": [
        "This observation leads us to our final expression of the EFE, which is not \"derived\" from bayesian first principles alone, but also informed or accomodated with operational wiring taken from considerations inherited by cybernetics, statistical mechanics, cognitive neuro-science, and fisiology:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "EFE \\triangleq - \\underbrace{\\mathbb{E}_{Q( \\bar{o} | \\bar{a})}\\left[D_{KL}\\left[Q(\\bar{s} | \\bar{o} , \\bar{a}) ||q_{\\bar{o}}(\\bar{s}\\mid \\bar{a}) \\right]\\right]}_{\\text{Epistemic gain}} - \\underbrace{\\mathbb{E}_{Q( \\bar{o}| \\bar{a})}\\left[ \\ln Q(\\bar{o} \\mid \\mathbf{C}) \\right]}_{\\text{Pragmatic Gain}}\\\\\\\\\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9853f5ba",
      "metadata": {
        "id": "9853f5ba"
      },
      "source": [
        "Please note there are other expressions for the EFE guy. You can see [this paper](https://arxiv.org/abs/2402.14460) if you are curious. [See also](https://arxiv.org/pdf/2004.08128) other valid \"schools of thought\" that prefer to minimise the epistemic gain.\n",
        "\n",
        "This tutorial aims by no means to be exhaustive, nor it pretends to be the last word. It just pretends to justify the following implementation of Deep Active Inference :)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdc1110a",
      "metadata": {
        "id": "bdc1110a"
      },
      "source": [
        "## Implementing Bootstrap value-based Deep Active Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5abae724",
      "metadata": {
        "id": "5abae724"
      },
      "source": [
        "To date, I know at least of three libraries for active inference: PyDMP, [RxInfer](https://rxinfer.com/), and [ActiveInference.jl](https://github.com/ComputationalPsychiatry/ActiveInference.jl)\n",
        "\n",
        "Our focus here is threefold:\n",
        "\n",
        "- go Deep, i.e. use Deep Neural Nets,\n",
        "- use value-based approximators of EFE (not policy based stuff)\n",
        "- use bootstrapping (not montecarlo methods)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The one-step back and forth free energy formulation:\n",
        "\n",
        "Though used as a way to evaluate policies or courses or sequences of actions, the VFE unctional can also be considered as a one-step backward retrospective account, and the EFE functional can also be considered as a one-step future prosprective account.\n",
        "\n",
        "> **In the following, we will place ourselves at time $t$. Suppose we are at time $t$. I.e. we observe $o_t$ as the result of $s_t$ (which is unknown) as a result of $a_{t-1}$ (which is known). We assume also that, at time $t$, $s_{t-1}$ is known.**\n",
        "\n",
        "Let's see what happens if $h=0$\n",
        "$$\n",
        "\\boxed{\n",
        "\\begin{aligned}\n",
        "\\underline{o} &\\triangleq o_{t:t}\\equiv o_{t} \\\\\n",
        "\\underline{a} &\\triangleq a_{t-1:t-1} \\equiv a_{t-1}\\\\\n",
        "\\bar{a} &\\triangleq  a_{t:t} \\equiv a_t\\\\\n",
        "\\bar{s} &\\triangleq  s_{t+1:t+1} \\equiv s_{t+1}\\\\\n",
        "\\bar{o} &\\triangleq  o_{t+1:t+1} \\equiv o_{t+1}\\\\\n",
        "\\end{aligned}\n",
        "}\n",
        "$$\n",
        "\n",
        "- **The retrospective matter: having done our last action $a_{t-1}$, we want to evaluate our current generative model's complexity and accuracy w.r.t. the observation we got, $o_t$, and the state we are actually able to infer, $s_t$**:\n",
        "\n",
        "The one-step-back retrospective VFE functional turns to be:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "VFE & =  \\underbrace{D_{KL} \\left[\\overbrace{q_{o_{t}}(s_t\\mid a_{t-1})}^{\\text{approx. posterior}} || \\overbrace{Q(s_t \\mid a_{t-1})}^{\\text{Prior}} \\right]}_{\\text{Complexity}} - \\overbrace{\\underbrace{\\mathbb{E}_{q_{o_{t}}(s_t\\mid a_{t-1})}\\left[ \\ln \\overbrace{Q(o_{t} \\mid s_t)}^{\\text{likelihood model}} \\right]}_{\\text{Accuracy}}}^{\\text{how likely  } o_t  \\text{  is under appox. post.}} \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "To implement such a thing, I need to known the prior $Q(s_t \\mid a_{t-1})$ which can either be something I bootstrap over (i.e. intialize + learn), or some sort of supervised learning, which necessarily involves waiting until time $t+1$. Let's use the second strategy: we'll have neural network implementations for the approximate posterior and the likelihood model. The parameters of those neural nets could be optimised down the negative gradients of the VFE functional, i.e., taking an action (in any way) to go to time $t+1$ so that we can get the actual value of $s_t$.\n",
        "\n",
        "(Moreover, notice that such a strategy would also permit us to additionally optimise the likelihood model with a more direct  supervised loss beyond VFE.)\n",
        "\n",
        "Now, how to choose actions?\n",
        "\n",
        "- **The prospective matter: we have to choose our next action $a_t$ that will generate $s_{t+1}$ that will generate $o_{t+1}$.**\n",
        "\n",
        "\n",
        "And one-step prospective EFE functional turns to be:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "EFE \\triangleq - \\underbrace{\\mathbb{E}_{Q( o_{t+1} | a_t)}\\left[D_{KL}\\left[Q(s_{t+1} | o_{t+1} , a_t) ||q_{o_{t+1}}(s_{t+1}\\mid a_t) \\right]\\right]}_{\\text{One-step epistemic gain}} - \\overbrace{\\underbrace{\\mathbb{E}_{Q( o_{t+1}| a_t)}\\left[ \\ln Q(o_{t+1} \\mid \\mathbf{C}) \\right]}_{\\text{One-step pragmatic Gain}}}^{\\text{TD-friendly}}\\\\\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "How to compute this?"
      ],
      "metadata": {
        "id": "rmbnTj9d5qZh"
      },
      "id": "rmbnTj9d5qZh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The one-step epistemic gain:"
      ],
      "metadata": {
        "id": "jbJun58GW2FF"
      },
      "id": "jbJun58GW2FF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\mathbb{E}_{Q( \\bar{o}, \\bar{s} | \\bar{a})}\\left[ \\ln q_{\\bar{o}}(\\bar{s}\\mid \\bar{a}) - \\ln Q(\\bar{s} | \\bar{o} , \\bar{a}) \\right]$$"
      ],
      "metadata": {
        "id": "ta_3ktqDLlLT"
      },
      "id": "ta_3ktqDLlLT"
    },
    {
      "cell_type": "markdown",
      "id": "559fac3f",
      "metadata": {
        "id": "559fac3f"
      },
      "source": [
        "This is our epistemic fella:\n",
        "\n",
        "$$\\mathbb{E}_{Q( o_{t+1} | a_t)}\\left[D_{KL}\\left[Q(s_{t+1} | o_{t+1} , a_t) ||q_{o_{t+1}}(s_{t+1}\\mid a_t) \\right]\\right]$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now the first distribution in the KL-div is also something we can estimate with the one-time sample we (assume) to have. I.e. if we assume that, at instant $t+1$, we know $s_t$, then we use $s_t$ and, with other assumptions (Gaussian forms for tractable divergences), we compute this KL divergence using the output of our variational poterior,  $\\hat{s}_{t+1} \\leftarrow \\mathbf{q}_{\\theta}( o_{t+1}, a_{t}) \\quad \\text{s.t.} \\quad \\hat{s}_{t+1} \\sim q_{o_{t+1}}(s_{t+1}\\mid a_t)$, and the actual $s_t$ we got:\n",
        "\n",
        "\n",
        "$$\n",
        "\\boxed{\n",
        "  \\begin{aligned}\n",
        "  &\\text{Approximated one-step epistemic gain:}\\\\\n",
        "&D_{KL}\\left[Q(s_t | o_t , a_{t-1}) || q_{o_t}(s_t\\mid a_{t-1}) \\right] \\approx \\frac{1}{2} \\left\\| s_t - \\mathbf{q}_{\\theta}( o_{t+1}, a_{t})\\right\\|_2^2\n",
        "\\end{aligned}\n",
        "}\n",
        "\\tag{AEG}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "O-kjy5OAZO-P"
      },
      "id": "O-kjy5OAZO-P"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Uox3EdvzGUGD"
      },
      "id": "Uox3EdvzGUGD"
    },
    {
      "cell_type": "markdown",
      "id": "wyM6cT2Iq6X-",
      "metadata": {
        "id": "wyM6cT2Iq6X-"
      },
      "source": [
        "### Implementing the variational approximation:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03k5fKecq-UW",
      "metadata": {
        "id": "03k5fKecq-UW"
      },
      "source": [
        "Let's leave aside for a moment the observation model. Our variational posterior will be implemented using a neural network $\\mathbf{q}_{\\theta}(o_{t+1}, a_{t})$, parametrised by $\\theta$, such that it's output will be the inferred state $\\hat{s}_{t+1}$:\n",
        "\n",
        "$$\n",
        "\\boxed{\n",
        "\\begin{aligned}\n",
        "\\text{Variational Posterior Net:}\\\\\n",
        "\\hat{s}_{t+1} \\leftarrow \\mathbf{q}_{\\theta}(o_{t+1}, a_{t})\\\\\n",
        "\\text{learn} \\quad \\theta^* \\quad \\text{such that}\\quad \\hat{s}_{t+1} \\sim q_{o_{t+1}}(s_{t+1}\\mid a_{t})\\\\\n",
        "\\end{aligned}\n",
        "}\\tag{VPN}\n",
        "$$\n",
        "\n",
        "The first line in (VPN) indicates that $\\hat{s}_{t+1}$ is a guess or inference about the real underlying state of the world, $s_{t+1}$, which is obtained as the output of a neural network $\\mathbf{q}_{\\theta}$, when the network is fed with a pair of inputs, namely, $o_{t+1}$ and $a_{t}$. Now the second line is interesting because it is saying that we want to learn some optimal values of the nets parameters, $\\theta^*$, such that we can assume that its outputs are equivalent to _sampling_ from the variational posterior distribution.\n",
        "\n",
        "NOTE: We won't use variational tricks in the following implementations, because our structure is already too much \"variational\" that we might not be able to converge... We will use direct outputs and assume those come from spherical Gaussians of unitary variance centred at the output values. That way variance is gonna be reduced and we will hopefully converge."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e85e66d1",
      "metadata": {
        "id": "e85e66d1"
      },
      "source": [
        "#### The \"value\" of policies and the Critic Net"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f05202b",
      "metadata": {
        "id": "2f05202b"
      },
      "source": [
        "The \"value\" of an action is a well-established concept in RL by which we rate the next action to take. Usually people talk about rating \"policies\" rather than actions. In RL jargon, policies are functions that output actions given states, in Act.Inf jargon, policies are just sequences of actions. Anyway, we are going to choose the NEXT action, $a_{t}$, i.e., our planing horizon will be just $h=1$, and guess what the rating score will be? Yep, you guessed, the EFE.\n",
        "\n",
        "\n",
        "\n",
        "We will create a neural network that approximates the EFE. Following the RL naming conventions, we'll call this fella the _critic network_ $\\mathbf{G}_{\\psi}(a_{t},s_{t})$, as it will estimate the EFE associated with each potentially next action $a_{t}$ given the current (inferred) state, $s_{t}$. The parameters of this neural network are denoted by $\\psi$.  \n",
        "\n",
        "So we create this neural net, we initialise it with whatever values. We will then optimise such values using our favorite learning paradigm (for now, lets suppose stochastic gradient descent). Above of that learning dynamics, the macro-structural way of optimising the $\\mathbf{G}$ guy will be through what is called \"temporal difference learning\", i.e., we will use some *real* one-step feedback from the world, add a our current $\\mathbf{G}$-based estimate of the rest of future steps, and use this thing as if it were \"the truth\", then optimise our $\\mathbf{G}$ to fit that \"thruth\". (This is what bootstrapping means, like lifting ourselves from the ground by pulling our bootstraps up). Anyways, it turns out to work:\n",
        "\n",
        "$$\n",
        "\\boxed{\n",
        "\\begin{aligned}\n",
        "&\\quad \\quad \\text{Bootstrapping/ TD-learning}\\\\\n",
        "\\underbrace{\\mathcal{G}(a_{t}, s_{t})}_{\\text{This is goundtruth EFE}} \\triangleq & \\underbrace{\\bar{\\mathcal{G}}(a_{t}, s_{t})}_{\\text{Real feedback}}  + \\underbrace{\\gamma}_{\\text{\"discount factor\"}} \\cdot \\underbrace{\\hat{\\mathbf{G}}_{\\hat{\\psi}}(a_{t+1}, s_{t+1})}_{\\text{Critic's net EFE estimate }}\\\\\\\\\n",
        "\\text{learn}  &\\quad \\psi^* \\quad \\text{such that:} \\quad \\hat{\\mathbf{G}}_{\\hat{\\psi}^*}(a_{t}, s_{t}) \\to \\mathcal{G}(a_{t}, s_{t})\n",
        "\\end{aligned}\n",
        "}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdfcec4c",
      "metadata": {
        "id": "bdfcec4c"
      },
      "source": [
        "But wait, what is this $\\bar{\\mathcal{G}}(a_{t}, s_{t})$ guy? IT is precisely a one-step a-posteriori computed value of the EFE. We take one action (maybe random or maybe doing something more interesting explained later) and then we see what happens, we see a new observation, we infer the new state, and we ask ourselves, what is the free energy of that single action I took?\n",
        "\n",
        "But wait a minute, one-step EFE isn't supposed to be VFE? what's the point in using the EFE monster if we run one-step!? We'll there's no point actually, unless we somehow \"encode\" the influence of a \"policy\" (in the RL sense, i.e. a kind of \"model of acting\") when computing the one-step EFE. By doing so, the estimate will *structurally* be one-step, but it's *inner value* will be \"infinitely\" step's ahead.\n",
        "\n",
        "Recall the one-step EFE formulation:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "EFE \\triangleq - \\underbrace{\\mathbb{E}_{Q( o_{t+1} | a_t)}\\left[D_{KL}\\left[Q(s_{t+1} | o_{t+1} , a_t) ||q_{o_{t+1}}(s_{t+1}\\mid a_t) \\right]\\right]}_{\\text{One-step epistemic gain}} - \\underbrace{\\mathbb{E}_{Q( o_{t+1}| a_t)}\\left[ \\ln Q(o_{t+1} \\mid \\mathbf{C}) \\right]}_{\\text{One-step pragmatic Gain}}\\\\\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We use that structure to compute our $\\bar{\\mathcal{G}}(a_{t}, s_{t-1})$ fella:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\\\\n",
        "&\\underbrace{\\bar{\\mathcal{G}}(a_{t}, s_{t})}_{\\text{one-step feedback}}\\triangleq \\\\\n",
        "& \\quad \\quad - \\underbrace{\\mathbb{E}_{Q( o_{t+1} | a_t)}\\left[D_{KL}\\left[Q(s_{t+1} | o_{t+1} , a_t) ||q_{o_{t+1}}(s_{t+1}\\mid a_t) \\right]\\right]}_{\\text{one-step epistemic gain}} \\\\\n",
        "& \\quad \\quad - \\underbrace{\\mathbb{E}_{Q( o_{t+1}| a_t)}\\left[ \\ln Q(o_{t+1} \\mid \\mathbf{C}) \\right]}_{\\text{one-step pragmatic Gain}}\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The Policy model:"
      ],
      "metadata": {
        "id": "iOVDp71bz4C8"
      },
      "id": "iOVDp71bz4C8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you remember how we used to factorise our generative/acting model?\n",
        "\n",
        "$$\n",
        "Q(s_{0:T}, a_{0:T-1}, o_{0:T}) = Q(s_0) \\prod_{t=0}^{T-1} Q(s_{t+1} \\mid s_t, a_t) \\cdot Q(a_t \\mid s_t) \\cdot \\prod_{t=0}^{T} Q(o_t \\mid s_t)\n",
        "$$\n",
        "\n",
        "Yep. There is this $Q(a_t \\mid s_t)$ guy, wich we will implement with another neural network. We will call this friend the _policy network_ $\\mathbf{Q}_{\\phi}(s_t)$, parametrised by $\\phi$, which outputs a distribution over plausible actions $a_t$ given the current (hypothetical/unknown/inferred) state $s_t$. This is our policy model, our way of acting, and this fella needs to enter the one-step EFE computation above if we want such a computation to be somehow meaningful."
      ],
      "metadata": {
        "id": "Nnmthf7oz864"
      },
      "id": "Nnmthf7oz864"
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\boxed{\n",
        "\\begin{aligned}\n",
        "\\text{Policy Net:}\\\\\n",
        "\\hat{a_t} \\leftarrow \\mathbf{Q}_{\\phi}(s_t)\\\\\n",
        "\\text{learn} \\quad \\phi^* \\quad \\text{such that}\\quad \\hat{a_t} \\sim Q(a_t \\mid s_t)\\\\\n",
        "\\end{aligned}\n",
        "}\\tag{PN}\n",
        "$$"
      ],
      "metadata": {
        "id": "os_TVwzV0N1x"
      },
      "id": "os_TVwzV0N1x"
    },
    {
      "cell_type": "markdown",
      "id": "0a38e367",
      "metadata": {
        "id": "0a38e367"
      },
      "source": [
        "#### The one-step EFE computation:\n",
        "\n",
        "Now that we have all of our building blocks, lets try to implement the one-step EFE feedback **(OS-EFE)**.\n",
        "\n",
        "Let's focus on the one-step epistemic gain:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SuA0qYAAoqP_",
      "metadata": {
        "id": "SuA0qYAAoqP_"
      },
      "source": [
        "The expected pragmatic Gain can be similarly approximated: the expectation is approximated by the current sample.\n",
        "$$\\mathbb{E}_{Q( o_t| a_{t-1})}\\left[ \\ln Q(o_t \\mid \\mathbf{C}) \\right] \\approx  \\ln Q(o_t \\mid \\mathbf{C})\n",
        "$$\n",
        "instead the normative observation is something we impose as a desideratum each time. Suppose we have a desired $o_t^*$, then, assuming Gaussian forms and thus tractable divergences, we'll have:\n",
        "\n",
        "$$\n",
        "\\boxed{\n",
        "  \\begin{aligned}\n",
        "  &\\text{Approximated one-step pragmatic gain:}\\\\\n",
        "&\\ln Q(o_t \\mid \\mathbf{C}) \\approx \\frac{1}{2} \\left\\| o_t - o_t^* \\right\\|_2^2\n",
        "\\end{aligned}\n",
        "}\n",
        "\\tag{APG}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "piuycrvDqGru",
      "metadata": {
        "id": "piuycrvDqGru"
      },
      "source": [
        "So the one-step ground-truth for EFE will be euqal to $-[(\\text{AEG}) + (\\text{APG})]$:\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "\\boxed{\n",
        "\\begin{aligned}\n",
        "&\\text{Approximated one-step EFE:}\\\\\n",
        "&\\bar{\\mathcal{G}}(a_t, s_t) \\approx - \\frac{1}{2} \\left\\| s_t - \\mathbf{q}_{\\theta}( o_t, a_{t-1})\\right\\|_2^2 - \\frac{1}{2} \\left\\| o_t - o_t^* \\right\\|_2^2\\\\\n",
        "\\end{aligned}\n",
        "}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c272da38",
      "metadata": {
        "id": "c272da38"
      },
      "source": [
        "#### Transition dynamics between states"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "644a6f8d",
      "metadata": {
        "id": "644a6f8d"
      },
      "source": [
        "Yes, our model factorisation also has this $Q(s_{t+1} \\mid s_t, a_t)$ guy, which will also be implemented with a neural net. We will call it the _transition network_, $\\mathbf{Q}_{\\theta}(s_{t+1} \\mid s_t, a_t)$, parametrised by $\\theta$, and it will output or predict the next state $s_{t+1}$ given the previous state $s_{t}$ and action taken $a_{t}$. This fella can be thought of as the \"perceptive\" part of our generative/agentic model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1449d77",
      "metadata": {
        "id": "e1449d77"
      },
      "source": [
        "### Temporal difference learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1550d59d",
      "metadata": {
        "id": "1550d59d"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}