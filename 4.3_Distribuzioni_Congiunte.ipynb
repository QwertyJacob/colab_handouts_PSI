{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "887ac92c",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/QwertyJacob/colab_handouts_PSI/blob/main/4.3_Distribuzioni_Congiunte.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a058e041",
   "metadata": {},
   "source": [
    "## 4.3 Distribuzioni Congiunte e Inferenza:\n",
    "__________________\n",
    "Adattamento da: \n",
    "- Probability and Statistics for Computer Scientists, M. Baron, CRC Press, 2014\n",
    "- Probability for Computer Scientists (CS109 Course Reader), C. Piech, Standford, 2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331b3c27",
   "metadata": {},
   "source": [
    "Spesso dovremmo considerare diverse variabili aleatorie simultaneamente. Possiamo per esempio dover pensare alla dimensione di una RAM e la velocità di una CPU, il prezzo di un computer e la sua capacità, la temperatura e l'umidità, le prestazioni tecniche e artistiche, ecc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b266a5",
   "metadata": {},
   "source": [
    "\n",
    "### 4.3.1 Distribuzione congiunta e distribuzioni marginali\n",
    "\n",
    "> **DEFINIZIONE 4.4**  **Distribuzione congiunta**. Se $ X $ e $ Y $ sono variabili aleatorie, allora la coppia $ (X, Y) $ è un **vettore aleatorio**. La sua distribuzione è chiamata **distribuzione congiunta** di $ X $ e $ Y $. Le distribuzioni individuali di $ X $ e $ Y $ sono quindi chiamate distribuzioni **marginali**.\n",
    "\n",
    "Anche se in questa sezione parliamo di due variabili aleatorie, tutti i concetti si estendono a un vettore $(X_1, X_2, \\ldots, X_n)$ di $ n $ componenti e alla sua distribuzione congiunta.\n",
    "\n",
    "In modo analogo al caso di una singola variabile, la distribuzione congiunta di un vettore è una collezione di probabilità che, per esempio, due variabili aleatorie $(X, Y)$ assumano, rispettivamente, un valore $(x, y)$. Quando diciamo che $X=x$ \"e\" $Y=y$, questo \"e\" indica l'intersezione, quindi la funzione di massa di probabilità congiunta di $ X $ e $ Y $. In linguaggio matematico si scrive:\n",
    "$$\n",
    "P(x, y) = P((X, Y) = (x, y)) = P(X = x \\cap Y = y).\n",
    "$$\n",
    "Ancora una volta, tutti gli eventi possibili  $ \\{(X, Y) = (x, y)\\} $ sono esaustivi e mutuamente esclusivi per coppie diverse $ (x, y) $, quindi\n",
    "$$\n",
    "\\sum_x \\sum_y P(x, y) = 1.\n",
    "$$\n",
    "\n",
    "La distribuzione congiunta di $ (X, Y) $ contiene informazioni complete sul comportamento di questo vettore aleatorio. In particolare, le funzioni di massa di probabilità marginali di $ X $ e $ Y $ possono essere ottenute dalla funzione di massa congiunta mediante la regola dell'addizione. (Figura 3.2)\n",
    "\n",
    "![Figure 3.2](figs/3.2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c5407",
   "metadata": {},
   "source": [
    "**(Regola dell'addizione)** \n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{aligned}\n",
    "&\\textbf{Regola dell'addizione}\\\\\n",
    "P_X(x) &= P(X = x) = \\sum_y P_{(X,Y)}(x, y)\\\\\n",
    "P_Y(y) &= P(Y = y) = \\sum_x P_{(X,Y)}(x, y)\\\\\n",
    "\\end{aligned}\n",
    "} \\tag{4.17}\n",
    "$$\n",
    "\n",
    "\n",
    "Cioè, per ottenere la funzione di massa di probabilità marginale di una variabile, si sommano le probabilità congiunte su tutti i valori dell'altra variabile.\n",
    "\n",
    "La Regola dell'addizione è illustrata nella Figura 3.2. Gli eventi $ \\{Y = y\\} $, per diversi valori di $ y $, partizionano lo spazio campione $ \\Omega $. Di conseguenza, le loro intersezioni con $ \\{X = x\\} $ partizionano l'evento $ \\{X = x\\} $ in parti mutuamente esclusive. Applicando la regola per l'unione di eventi mutuamente esclusivi, cioè la formula $(2.4)$, le relative probabilità vanno sommate. Queste probabilità sono proprio $ P_{(X,Y)}(x, y) $.\n",
    "\n",
    "In generale, la distribuzione congiunta non può essere calcolata a partire dalle distribuzioni marginali, perché queste ultime non contengono informazioni sulle relazioni tra le variabili aleatorie. Ad esempio, le distribuzioni marginali non permettono di stabilire se le variabili $ X $ e $ Y $ siano indipendenti o dipendenti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2594f1a2",
   "metadata": {},
   "source": [
    "### 4.3.2 Indipendenza tra variabili aleatorie:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75b8485",
   "metadata": {},
   "source": [
    "> **DEFINIZIONE 4.5** **Due variabili aleatorie $ X $ e $ Y $ sono indipendenti se**\n",
    "$$\n",
    "P_{(X,Y)}(x, y) = P_X(x) P_Y(y)\n",
    "$$\n",
    "per tutti i valori di $ x $ e $ y $. Questo significa che gli eventi $ \\{X = x\\} $ e $ \\{Y = y\\} $ sono indipendenti per ogni $ x $ e $ y $; in altre parole, le variabili $ X $ e $ Y $ assumono i propri valori indipendentemente l'una dall'altra.\n",
    "\n",
    "Negli esercizi, per dimostrare l'indipendenza di $ X $ e $ Y $, dobbiamo verificare se la funzione di massa di probabilità congiunta si fattorizza nel prodotto delle funzioni di massa marginali per tutte le coppie $ (x, y) $. Per dimostrare la dipendenza, è sufficiente fornire un controesempio, cioè una coppia $ (x, y) $ tale che $ P(x, y) \\neq P_X(x) P_Y(y) $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888a3718",
   "metadata": {},
   "source": [
    "**Esempio BONUS** Un programma è composto da due moduli. Il numero di errori, $ X $, nel primo modulo e il numero di errori, $ Y $, nel secondo modulo hanno la seguente distribuzione congiunta:\n",
    "\n",
    "$$\n",
    "P(0, 0) = P(0, 1) = P(1, 0) = 0.2, \\quad P(1, 1) = P(1, 2) = P(1, 3) = 0.1, \\quad P(0, 2) = P(0, 3) = 0.05.\n",
    "$$\n",
    "\n",
    "Determinare:  \n",
    "(a) le distribuzioni marginali di $ X $ e $ Y $,  \n",
    "(b) la probabilità di nessun errore nel primo modulo,  \n",
    "(c) la distribuzione del numero totale di errori nel programma,  \n",
    "(d) se gli errori nei due moduli si verificano in modo indipendente.\n",
    "\n",
    "**Soluzione.** È conveniente organizzare la funzione di massa di probabilità congiunta di $ X $ e $ Y $ in una tabella. Sommando per righe e per colonne, otteniamo le funzioni di massa marginali:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cccc|c}\n",
    "x \\backslash y & 0 & 1 & 2 & 3 & P_X(x) \\\\\n",
    "\\hline\n",
    "0 & 0.20 & 0.20 & 0.05 & 0.05 & 0.50 \\\\\n",
    "1 & 0.20 & 0.10 & 0.10 & 0.10 & 0.50 \\\\\n",
    "\\hline\n",
    "P_Y(y) & 0.40 & 0.30 & 0.15 & 0.15 & 1.00 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "Questo risolve il punto (a).\n",
    "\n",
    "(b) $ P_X(0) = 0.50 $.\n",
    "\n",
    "(c) Sia $ Z = X + Y $ il numero totale di errori. Per trovare la distribuzione di $ Z $, identifichiamo innanzitutto i suoi possibili valori, quindi calcoliamo la probabilità di ciascun valore. Si osserva che $ Z $ può assumere valori da $ 0 $ a $ 4 $. Allora:\n",
    "\n",
    "$$\n",
    "P_Z(0) = P(X + Y = 0) = P(X = 0 \\cap Y = 0) = P(0, 0) = 0.20,\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_Z(1) = P(X = 0 \\cap Y = 1) + P(X = 1 \\cap Y = 0) = P(0, 1) + P(1, 0) = 0.20 + 0.20 = 0.40,\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_Z(2) = P(0, 2) + P(1, 1) = 0.05 + 0.10 = 0.15,\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_Z(3) = P(0, 3) + P(1, 2) = 0.05 + 0.10 = 0.15,\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_Z(4) = P(1, 3) = 0.10.\n",
    "$$\n",
    "\n",
    "È utile verificare che la somma delle probabilità sia 1:\n",
    "\n",
    "$$\n",
    "\\sum_z P_Z(z) = 0.20 + 0.40 + 0.15 + 0.15 + 0.10 = 1.00.\n",
    "$$\n",
    "\n",
    "(d) Per stabilire se $ X $ e $ Y $ sono indipendenti, verifichiamo se la funzione di massa congiunta si fattorizza nel prodotto delle marginali. Osserviamo che:\n",
    "\n",
    "$$\n",
    "P_{(X,Y)}(0, 0) = 0.20, \\quad P_X(0) P_Y(0) = (0.50)(0.40) = 0.20.\n",
    "$$\n",
    "\n",
    "La condizione è soddisfatta per questa coppia. Procediamo con un'altra coppia:\n",
    "\n",
    "$$\n",
    "P_{(X,Y)}(0, 1) = 0.20, \\quad P_X(0) P_Y(1) = (0.50)(0.30) = 0.15.\n",
    "$$\n",
    "\n",
    "Poiché $ 0.20 \\neq 0.15 $, abbiamo trovato una coppia $ (x, y) $ che viola la condizione di indipendenza. Non è quindi necessario proseguire. Le variabili $ X $ e $ Y $ sono dipendenti.\n",
    "\n",
    "Pertanto, il numero di errori nei due moduli è dipendente. ♦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a889612",
   "metadata": {},
   "source": [
    "### 4.3.3 Marginalizzazione"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c883f",
   "metadata": {},
   "source": [
    "Un'importante intuizione riguardo i modelli probabilistici con molte variabili aleatorie è che \"la distribuzione congiunta contiene informazioni complete\". A partire dalla distribuzione congiunta, è possibile calcolare tutte le probabilità riguardanti le variabili aleatorie incluse nel modello. Questa sezione è un esempio di tale intuizione.\n",
    "\n",
    "La domanda centrale di questa sezione è: data una distribuzione congiunta, come è possibile calcolare la probabilità delle singole variabili aleatorie?\n",
    "\n",
    "**Marginalizzazione a partire da due variabili aleatorie**\n",
    "\n",
    "Per iniziare, consideriamo due variabili aleatorie $ X $ e $ Y $. Se viene fornita la distribuzione congiunta, come si può calcolare $ P(X = x) $? Ricordiamo che, se si dispone della distribuzione congiunta, si conosce la probabilità $ P(X = x, Y = y) $ per ogni valore di $ x $ e $ y $. Già possediamo una tecnica per calcolare $ P(X = x) $ a partire dalla distribuzione congiunta: possiamo usare il **Teorema della Probabilità Totale**! In questo caso, gli eventi $ Y = y $ costituiscono gli \"eventi di sfondo\":\n",
    "\n",
    "$$\n",
    "P(X = x) = \\sum_y P(X = x, Y = y)\n",
    "$$\n",
    "\n",
    "Notiamo che, per applicare il Teorema della Probabilità Totale, è necessario che gli eventi $ Y = y $ siano mutualmente esclusivi e che valga $ \\sum_y P(Y = y) = 1 $. Entrambe le condizioni sono soddisfatte.\n",
    "\n",
    "Se volessimo calcolare $ P(Y = y) $, potremmo nuovamente utilizzare la Legge della Probabilità Totale, usando questa volta i possibili valori di $ X $ come eventi di sfondo:\n",
    "\n",
    "$$\n",
    "P(Y = y) = \\sum_x P(X = x, Y = y)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b0bee7",
   "metadata": {},
   "source": [
    "> **Esempio BONUS: Numero preferito** Consideriamo la seguente distribuzione congiunta per le variabili aleatorie $ X $ e $ Y $, dove $ X $ è la cifra binaria preferita da una persona e $ Y $ è il loro anno accademico a Stanford. Di seguito è riportata una distribuzione congiunta reale proveniente da un'edizione precedente del corso:\n",
    "\n",
    "|                   | $ X = 0 $ | $ X = 1 $ |\n",
    "|-------------------|-----------|-----------|\n",
    "| $ Y = \\text{Frosh} $   | 0.01      | 0.13      |\n",
    "| $ Y = \\text{Soph} $    | 0.05      | 0.33      |\n",
    "| $ Y = \\text{Junior} $  | 0.04      | 0.21      |\n",
    "| $ Y = \\text{Senior} $  | 0.03      | 0.12      |\n",
    "| $ Y = 5+ $             | 0.02      | 0.06      |\n",
    "\n",
    "Qual è la probabilità che la cifra preferita da uno studente sia $ 0 $, ovvero $ P(X = 0) $? Possiamo utilizzare la Legge della Probabilità Totale (LOTP) per calcolare questa probabilità:\n",
    "\n",
    "$$\n",
    "P(X = 0) = \\sum_y P(X = 0, Y = y)\n",
    "$$\n",
    "\n",
    "Espandendo la sommatoria su tutti i possibili valori di $ Y $:\n",
    "\n",
    "$$\n",
    "P(X = 0) = P(X = 0, Y = \\text{Frosh}) + P(X = 0, Y = \\text{Soph}) + P(X = 0, Y = \\text{Junior}) + P(X = 0, Y = \\text{Senior}) + P(X = 0, Y = 5+)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(X = 0) = 0.01 + 0.05 + 0.04 + 0.03 + 0.02 = 0.15\n",
    "$$\n",
    "\n",
    "Pertanto, la probabilità che la cifra preferita da uno studente sia $ 0 $ è $ 0.15 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f560d73e",
   "metadata": {},
   "source": [
    "**Marginalizzazione con più variabili**\n",
    "\n",
    "L'idea di marginalizzazione può essere estesa a distribuzioni congiunte con più di due variabili aleatorie. Consideriamo tre variabili aleatorie $ X $, $ Y $ e $ Z $. Possiamo marginalizzare rispetto a una qualsiasi di queste variabili:\n",
    "\n",
    "$$\n",
    "P(X = x) = \\sum_{y,z} P(X = x, Y = y, Z = z)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(Y = y) = \\sum_{x,z} P(X = x, Y = y, Z = z)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(Z = z) = \\sum_{x,y} P(X = x, Y = y, Z = z)\n",
    "$$\n",
    "\n",
    "\n",
    "**Notazione: Doppia sommatoria**\n",
    "\n",
    "Nel caso sopra, la notazione con doppia sommatoria:\n",
    "\n",
    "$$\n",
    "\\sum_{y,z}\n",
    "$$\n",
    "\n",
    "può essere scritta in modo equivalente come:\n",
    "\n",
    "$$\n",
    "\\sum_y \\sum_z\n",
    "$$\n",
    "\n",
    "e indica che stiamo sommando su tutti i possibili valori di $ y $ e $ z $. Ad esempio, se $ Y $ è una variabile aleatoria con 3 possibili esiti e $ Z $ ne ha 4, allora $ \\sum_{y,z} $ implica sommare su tutte e 12 le combinazioni possibili di $ y $ e $ z $.\n",
    "\n",
    "\n",
    "Di seguito è riportato un esempio in codice. Supponiamo di avere una funzione `joint(x, y, z)` che restituisce $ P(X = x, Y = y, Z = z) $, e che ciascuna tra $ X $, $ Y $ e $ Z $ possa assumere valori nell'insieme $ \\{0, 1, 2, 3, 4\\} $. Allora, per calcolare la distribuzione marginale $ P(X = x) $ per un dato valore $ x $, possiamo scrivere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63f2855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint(x, y, z):\n",
    "    # supponendo di avere equiprobabilità\n",
    "    return 1 / 125\n",
    "\n",
    "def marginal_X(x):\n",
    "    total = 0.0\n",
    "    for y in range(5):      # y in {0,1,2,3,4}\n",
    "        for z in range(5):  # z in {0,1,2,3,4}\n",
    "            total += joint(x, y, z)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950b5fac",
   "metadata": {},
   "source": [
    "Questo codice implementa la marginalizzazione:\n",
    "\n",
    "$$\n",
    "P(X = x) = \\sum_{y=0}^4 \\sum_{z=0}^4 P(X = x, Y = y, Z = z)\n",
    "$$\n",
    "\n",
    "Analogamente, si possono definire funzioni per $ P(Y = y) $ e $ P(Z = z) $ sommando sulle altre due variabili.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a133e8e",
   "metadata": {},
   "source": [
    "## Inferenza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46212361",
   "metadata": {},
   "source": [
    "\n",
    "Fino ad ora abbiamo gettato le basi su come rappresentare modelli probabilistici con più variabili aleatorie. Questi modelli sono particolarmente utili perché ci consentono di eseguire un'operazione chiamata \"inferenza\", in cui aggiorniamo la nostra credenza su una variabile aleatoria nel modello, condizionata su nuove informazioni riguardo a un'altra. In generale, l'inferenza è difficile! Infatti, è stato dimostrato che, nel caso peggiore, l'operazione di inferenza può essere NP-Hard, dove $ n $ è il numero di variabili aleatorie [[1]](https://www.sciencedirect.com/science/article/pii/000437029090060D).\n",
    "\n",
    "Innanzitutto, ci eserciteremo con due variabili aleatorie (in questa sezione).\n",
    "\n",
    "In precedenza, abbiamo esaminato le probabilità condizionate per gli eventi. Il primo compito nell'inferenza è comprendere come combinare le probabilità condizionate e le variabili aleatorie. Le equazioni per i casi, sia discreto che continuo, sono estensioni intuitive della nostra comprensione della probabilità condizionata:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb18aa",
   "metadata": {},
   "source": [
    "**La condizionata nel caso discreto**\n",
    "\n",
    "Il caso discreto, in cui ogni variabile aleatoria nel modello è discreta, è una combinazione diretta di quanto già noto sulla probabilità condizionata (studiata nel contesto degli eventi). Ricordiamo che ogni operatore relazionale (=, <, >, $\\geq$, $\\leq$) applicato a una variabile aleatoria definisce un evento. Di conseguenza, le regole della probabilità condizionata si applicano direttamente.\n",
    "\n",
    "**Def: Funzione di massa di probabilità (PMF) condizionata nel caso discreto.**\n",
    "\n",
    "Siano $ X $ e $ Y $ due variabili aleatorie discrete. Allora:\n",
    "\n",
    "$$\n",
    "P(X = x \\mid Y = y) = \\frac{P(X = x, Y = y)}{P(Y = y)}\n",
    "$$\n",
    "\n",
    "**Def: Teorema di Bayes nel caso di variabili aleatorie discrete.**\n",
    "\n",
    "$$\n",
    "P(X = x \\mid Y = y) = \\frac{P(Y = y \\mid X = x) P(X = x)}{P(Y = y)}\n",
    "$$\n",
    "\n",
    "In presenza di molteplici variabili aleatorie, risulta sempre più utile adottare una notazione compatta. La definizione precedente è identica alla seguente notazione, in cui un simbolo minuscolo come $ x $ è una forma abbreviata per indicare l'evento $ X = x $:\n",
    "\n",
    "$$\n",
    "P(x \\mid y) = \\frac{P(x, y)}{P(y)}\n",
    "$$\n",
    "\n",
    "Questa notazione compatta è comunemente usata nei modelli probabilistici complessi per rendere le espressioni più concise, mantenendo al contempo il significato probabilistico ben definito.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809135ef",
   "metadata": {},
   "source": [
    "La definizione di probabilità condizionata si applica a ogni evento, e pertanto possiamo scrivere anche probabilità condizionate utilizzando le funzioni di ripartizione (CDF) nel caso discreto:\n",
    "\n",
    "$$\n",
    "P(X \\leq a \\mid Y = y) = \\frac{P(X \\leq a, Y = y)}{P(Y = y)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{\\sum_{x \\leq a} P(X = x, Y = y)}{P(Y = y)}\n",
    "$$\n",
    "\n",
    "Ecco un risultato interessante: l'ultima espressione può essere riscritta mediante un'opportuna manipolazione. Possiamo far sì che la sommatoria si estenda sull'intera frazione:\n",
    "\n",
    "$$\n",
    "P(X \\leq a \\mid Y = y) = \\sum_{x \\leq a} \\frac{P(X = x, Y = y)}{P(Y = y)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sum_{x \\leq a} P(X = x \\mid Y = y)\n",
    "$$\n",
    "\n",
    "Infatti, diventa immediato tradurre le regole della probabilità (come il teorema di Bayes, la legge della probabilità totale, ecc.) nel linguaggio delle variabili aleatorie discrete: è sufficiente ricordare che ogni operatore relazionale applicato a una variabile aleatoria definisce un evento. Di conseguenza, tutte le regole valide per gli eventi si applicano direttamente alle espressioni che coinvolgono variabili aleatorie.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59504195",
   "metadata": {},
   "source": [
    "**Mischiando tra variabili discrete e continue**\n",
    "\n",
    "Cosa succede quando vogliamo ragionare su variabili aleatorie continue utilizzando le regole della probabilità (come il teorema di Bayes, la legge della probabilità totale, la regola della catena, ecc.)? C'è una risposta pratica semplice: le regole continuano a valere, ma dobbiamo sostituire la terminologia probabilistica con le funzioni di densità di probabilità. Vediamo un esempio concreto con il teorema di Bayes nel caso di una variabile aleatoria continua e una discreta.\n",
    "\n",
    "**Def: Teorema di Bayes con variabili miste (discreta e continua).**\n",
    "\n",
    "Sia $ X $ una variabile aleatoria continua e sia $ N $ una variabile aleatoria discreta. Le probabilità condizionate di $ X $ dato $ N $ e di $ N $ dato $ X $ sono rispettivamente:\n",
    "\n",
    "$$\n",
    "f(X = x \\mid N = n) = \\frac{P(N = n \\mid X = x) f(X = x)}{P(N = n)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(N = n \\mid X = x) = \\frac{f(X = x \\mid N = n) P(N = n)}{f(X = x)}\n",
    "$$\n",
    "\n",
    "Queste equazioni possono sembrare complicate perché mescolano densità di probabilità e probabilità. Perché dovremmo ritenere che siano corrette?\n",
    "\n",
    "Innanzitutto, osserviamo che ogni volta che la variabile aleatoria a sinistra del condizionamento è continua, usiamo una densità $ f $; quando invece è discreta, usiamo una probabilità $ P $. Questa distinzione è fondamentale.\n",
    "\n",
    "Questo risultato può essere ottenuto osservando che:\n",
    "\n",
    "$$\n",
    "P(X = x) \\approx f(X = x) \\cdot \\epsilon_x\n",
    "$$\n",
    "\n",
    "nel limite in cui $ \\epsilon_x \\to 0 $. Infatti, per ottenere una probabilità da una funzione di densità, dobbiamo integrare la densità su un intervallo. Se vogliamo approssimare la probabilità che $ X $ cada in un piccolo intervallo attorno a $ x $, possiamo considerare l'area di un rettangolo di altezza $ f(X = x) $ e larghezza $ \\epsilon_x $:\n",
    "\n",
    "$$\n",
    "P(x \\leq X < x + \\epsilon_x) \\approx f(X = x) \\cdot \\epsilon_x\n",
    "$$\n",
    "\n",
    "Al diminuire di $ \\epsilon_x $, questa approssimazione diventa sempre più accurata. Nel limite, la probabilità che una variabile continua assuma un valore specifico è zero, ma il rapporto tra probabilità infinitesime e densità rimane ben definito, permettendo di combinare in modo coerente probabilità e densità nelle formule di inferenza.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a208ce",
   "metadata": {},
   "source": [
    "![Figure 4.5](figs/4.5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a1820e",
   "metadata": {},
   "source": [
    "Un valore di $ \\epsilon_x $ è problematico se rimane all'interno di una formula. Tuttavia, se riusciamo a semplificarlo, possiamo ottenere un'equazione utilizzabile. Questa è l'intuizione chiave utilizzata per derivare le regole della probabilità nel caso in cui siano presenti una o più variabili aleatorie continue.\n",
    "\n",
    "Ancora una volta, sia $ X $ una variabile aleatoria continua e sia $ N $ una variabile aleatoria discreta. Applichiamo il teorema di Bayes:\n",
    "\n",
    "$$\n",
    "P(N = n \\mid X = x) = \\frac{P(X = x \\mid N = n) P(N = n)}{P(X = x)}\n",
    "\\quad \\text{(Teorema di Bayes)}\n",
    "$$\n",
    "\n",
    "Sostituiamo le probabilità con le densità moltiplicate per $ \\epsilon_x $, come approssimazione valida per intervalli infinitesimi:\n",
    "\n",
    "$$\n",
    "P(X = x \\mid N = n) \\approx f(X = x \\mid N = n) \\cdot \\epsilon_x\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(X = x) \\approx f(X = x) \\cdot \\epsilon_x\n",
    "$$\n",
    "\n",
    "Sostituendo nella formula di Bayes:\n",
    "\n",
    "$$\n",
    "P(N = n \\mid X = x) = \\frac{f(X = x \\mid N = n) \\cdot \\epsilon_x \\cdot P(N = n)}{f(X = x) \\cdot \\epsilon_x}\n",
    "$$\n",
    "\n",
    "Ora, semplifichiamo $ \\epsilon_x $ al numeratore e al denominatore:\n",
    "\n",
    "$$\n",
    "P(N = n \\mid X = x) = \\frac{f(X = x \\mid N = n) \\cdot P(N = n)}{f(X = x)}\n",
    "$$\n",
    "\n",
    "La quantità $ \\epsilon_x $ si cancella, lasciando un'espressione ben definita che combina densità di probabilità e probabilità. Questo risultato mostra come le regole della probabilità — come il teorema di Bayes — possano essere estese in modo coerente al caso misto (discreto/continuo), a patto di trattare correttamente le densità quando si ha a che fare con variabili continue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a67dbbc",
   "metadata": {},
   "source": [
    "Questa strategia si applica non solo al teorema di Bayes, ma a tutte le principali regole della probabilità. Ad esempio, ecco una versione della **Legge della Probabilità Totale** nel caso in cui $ X $ sia una variabile aleatoria continua e $ N $ sia una variabile aleatoria discreta:\n",
    "\n",
    "$$\n",
    "f(X = x) = \\sum_{n \\in N} f(X = x \\mid N = n) P(N = n)\n",
    "$$\n",
    "\n",
    "Questa equazione esprime la densità marginale di $ X $ come una media pesata delle densità condizionate $ f(X = x \\mid N = n) $, con pesi dati dalle probabilità $ P(N = n) $. È l'analogo continuo della legge della probabilità totale per eventi, adattato al caso misto.\n",
    "\n",
    "Anche in questo caso, la derivazione si basa sull'approssimazione:\n",
    "\n",
    "$$\n",
    "P(X = x) \\approx f(X = x) \\cdot \\epsilon_x\n",
    "$$\n",
    "\n",
    "e sull'applicazione della legge della probabilità totale agli eventi discreti $ N = n $, seguita dalla semplificazione di $ \\epsilon_x $ in tutti i termini. Il risultato è un'espressione coerente e utilizzabile per l'inferenza in modelli che combinano variabili aleatorie discrete e continue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90889e",
   "metadata": {},
   "source": [
    "> **Esempio BONUS:**  Alla nascita, il peso delle elefantine femmine segue una distribuzione gaussiana con media $ 160 $ kg e deviazione standard $ 7 $ kg. Alla nascita, il peso dei cuccioli maschi segue una distribuzione gaussiana con media $ 165 $ kg e deviazione standard $ 3 $ kg. L'unica informazione che si ha su un elefante appena nato è che pesa $ 163 $ kg. Qual è la probabilità che sia una femmina?\n",
    "\n",
    "**Risposta:** Sia $ G $ una variabile aleatoria indicatrice che indica se l'elefante è una femmina:\n",
    "\n",
    "- $ G = 1 $: l'elefante è una femmina\n",
    "- $ G = 0 $: l'elefante è un maschio\n",
    "\n",
    "Assumiamo che $ G \\sim \\text{Bernoulli}(p = 0.5) $, ovvero che a priori la probabilità che un elefante appena nato sia femmina o maschio sia pari al 50%.\n",
    "\n",
    "Sia $ X $ la variabile aleatoria che rappresenta il peso alla nascita. Le distribuzioni condizionate sono:\n",
    "\n",
    "- $ X \\mid G = 1 \\sim \\mathcal{N}(\\mu = 160, \\sigma^2 = 49) $\n",
    "- $ X \\mid G = 0 \\sim \\mathcal{N}(\\mu = 165, \\sigma^2 = 9) $\n",
    "\n",
    "Vogliamo calcolare la probabilità che l'elefante sia una femmina dato il peso osservato, ovvero:\n",
    "\n",
    "$$\n",
    "P(G = 1 \\mid X = 163)\n",
    "$$\n",
    "\n",
    "Applichiamo il **teorema di Bayes** per variabili miste (discreta $ G $, continua $ X $):\n",
    "\n",
    "$$\n",
    "P(G = 1 \\mid X = 163) = \\frac{\\epsilon \\, f(X = 163 \\mid G = 1) P(G = 1)}{\\epsilon \\, f(X = 163)}\n",
    "$$\n",
    "\n",
    "Il denominatore può essere sviluppato usando la **legge della probabilità totale per variabili continue**:\n",
    "\n",
    "$$\n",
    "f(X = 163) = f(X = 163 \\mid G = 1) P(G = 1) + f(X = 163 \\mid G = 0) P(G = 0)\n",
    "$$\n",
    "\n",
    "Dato che $ P(G = 1) = P(G = 0) = 0.5 $ abbiamo:\n",
    "\n",
    "$$\n",
    "P(G = 1 \\mid X = 163) = \\frac{\\frac12 \\epsilon \\, f(X = 163 \\mid G = 1)}{\\frac12 \\epsilon \\left[f(X = 163 \\mid G = 1) + f(X = 163 \\mid G = 0)\\right]}\n",
    "$$\n",
    "\n",
    "Semplificando $\\frac12 \\epsilon $ sopra e sotto:\n",
    "$$\n",
    "P(G = 1 \\mid X = 163) = \\frac{ f(X = 163 \\mid G = 1)}{f(X = 163 \\mid G = 1) + f(X = 163 \\mid G = 0)}\n",
    "$$\n",
    "\n",
    "\n",
    "Calcoliamo ora le due densità gaussiane.\n",
    "\n",
    "1. Densità per femmina: $ f(X = 163 \\mid G = 1) $\n",
    "\n",
    "$$\n",
    "f(X = 163 \\mid G = 1) = \\frac{1}{7 \\sqrt{2\\pi}} e^{\\left( -\\frac{1}{2} \\left( \\frac{163 - 160}{7} \\right)^2 \\right)} \\approx 0.0520\n",
    "$$\n",
    "\n",
    "2. Densità per maschio: $ f(X = 163 \\mid G = 0) $\n",
    "\n",
    "$$\n",
    "f(X = 163 \\mid G = 0) = \\frac{1}{3 \\sqrt{2\\pi}} e^{\\left( -\\frac{1}{2} \\left( \\frac{163 - 165}{3} \\right)^2 \\right)} \\approx 0.1065 \n",
    "$$\n",
    "\n",
    "\n",
    "Ora sostituiamo nella formula di Bayes:\n",
    "\n",
    "$$\n",
    "P(G = 1 \\mid X = 163) = \\frac{0.0520}{0.0520 + 0.1065}  \\approx 0.328\n",
    "$$\n",
    "\n",
    "**Risposta finale:** La probabilità che l'elefante appena nato sia una femmina, dato che pesa $ 163 $ kg, è approssimativamente $ 0.328 $, ovvero **32.8%**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
